#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pentru verificarea È™i È™tergerea automatÄƒ a folderelor
care conÈ›in fiÈ™iere deja prezente pe Internet Archive.
FoloseÈ™te API-ul JSON al Archive.org pentru verificare rapidÄƒ.

È˜terge fiÈ™ierul archive_cleanup_state.json Ã®nainte de a rula scriptul, sau adaugÄƒ aceastÄƒ funcÈ›ie È™i apeleaz-o:

de ce nu imi spui niciodata unde anume sa adaug bucatile de cod? Daca este o functie noua care nu exista in codul precedent, si deci nu am cum sa o caut ni cod, te rog frumos sa imi spui unde anume sa o adaug in cod. DUpa ce linie sau intre ce linii, sau dupa ce functie sau intre ce functiii.
foarte bine. Pe viitor sa scrii functia noua, si sa-mi spui ca aici: AdaugÄƒ funcÈ›ia  def reset_all_state(self): - imediat DUPÄ‚ funcÈ›ia save_state È™i ÃNAINTE de funcÈ›ia clean_title_for_search . Asa e cel mai usor de inteles. Dar daca imi dau sa adaug linii noi intr-o functie, imi spui exact intre ce linii sa introduc acele linii noi, sau de unde pana unde trebuie sa le inlocuiesc.
"""

import os
import re
import time
import shutil
import json
import logging
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# ============= CONFIGURÄ‚RI =============
ARCHIVE_PATH = Path(r"g:\ARHIVA\B")
DELAY_BETWEEN_SEARCHES = 2  # secunde Ã®ntre cÄƒutÄƒri pentru a nu suprasolicita serverul
STATE_FILE = Path("archive_cleanup_state.json")
LOG_FILE = Path(f"archive_cleanup_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
RELEVANT_EXTENSIONS = ['.pdf', '.epub', '.mobi', '.djvu', '.docx', '.doc', '.lit', '.rtf']
MAX_API_ATTEMPTS = 3
API_TIMEOUT = 15

# ============= CONFIGURARE LOGGER =============
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ArchiveDuplicateChecker")


class ArchiveDuplicateChecker:
    def __init__(self):
        self.state = self.load_state()
        self.deleted_count = 0
        self.checked_count = 0
        self.error_count = 0
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArchiveDuplicateChecker/1.0 (Python Script)'
        })

    def load_state(self) -> Dict[str, Any]:
        """ÃncarcÄƒ starea salvatÄƒ anterior"""
        default_state = {
            "processed_folders": [],
            "deleted_folders": [],
            "last_processed": None,
            "stats": {
                "total_checked": 0,
                "total_deleted": 0,
                "total_space_saved_mb": 0.0
            }
        }

        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    state = json.load(f)

                    # IMPORTANT: AsigurÄƒ compatibilitate cu versiuni anterioare
                    # AdaugÄƒ cheile lipsÄƒ din starea veche
                    if "deleted_folders" not in state:
                        state["deleted_folders"] = []

                    if "stats" not in state:
                        state["stats"] = default_state["stats"]

                    if "processed_folders" not in state:
                        state["processed_folders"] = []

                    if "last_processed" not in state:
                        state["last_processed"] = None

                    logger.info(f"ğŸ“‹ Stare Ã®ncÄƒrcatÄƒ: {len(state.get('processed_folders', []))} foldere procesate anterior")
                    return state
            except Exception as e:
                logger.warning(f"âš ï¸ Nu s-a putut Ã®ncÄƒrca starea: {e}")
                logger.info("ğŸ“ Creez o stare nouÄƒ...")

        return default_state

    def save_state(self):
        """SalveazÄƒ starea curentÄƒ"""
        self.state["last_processed"] = datetime.now().isoformat()
        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)
            logger.debug("ğŸ’¾ Stare salvatÄƒ cu succes")
        except Exception as e:
            logger.error(f"âŒ Eroare la salvarea stÄƒrii: {e}")

    def reset_all_state(self):
        """ReseteazÄƒ complet toatÄƒ starea salvatÄƒ"""
        logger.warning("ğŸ”„ RESETARE COMPLETÄ‚ A STÄ‚RII!")
        self.state = {
            "processed_folders": [],
            "deleted_folders": [],
            "last_processed": None,
            "stats": {
                "total_checked": 0,
                "total_deleted": 0,
                "total_space_saved_mb": 0.0
            }
        }
        self.save_state()


    def clean_title_for_search(self, filename: str) -> str:
        """
        CurÄƒÈ›Äƒ numele fiÈ™ierului pentru cÄƒutare pe Internet Archive:
        - EliminÄƒ extensia
        - EliminÄƒ versiunile (v.0.9, v.0.9.8.5-161, v.0.9.8 MMXII, etc.)
        - EliminÄƒ sufixele (scan, ctrl, retail, cop1, etc.)
        - EliminÄƒ parantezele È™i conÈ›inutul lor
        - EliminÄƒ numerele de la Ã®nceput din titlu
        - PÄƒstreazÄƒ doar formatul "Nume, Prenume - Titlu"
        """
        # EliminÄƒ extensia
        name = Path(filename).stem

        logger.debug(f"ğŸ”§ CurÄƒÈ›are titlu original: {filename}")

        # EliminÄƒ sufixele de tipul _202508, _20250804, etc.
        name = re.sub(r'[_-]\d{6,8}$', '', name)

        # EliminÄƒ parantezele rotunde È™i tot conÈ›inutul lor
        name = re.sub(r'\s*\([^)]*\)', '', name)

        # EliminÄƒ È™i parantezele pÄƒtrate dacÄƒ existÄƒ
        name = re.sub(r'\s*\[[^\]]*\]', '', name)

        # EliminÄƒ versiunile complexe care pot apÄƒrea dupÄƒ " - "
        # AcoperÄƒ: v.0.9, v.0.9.8.5-161, v.0.9.8 MMXII, v.1.0, etc.
        name = re.sub(r'\s*[-â€“]\s*[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?(?:\s*[-â€“]\s*\d+)?$', '', name)

        # EliminÄƒ versiunile care pot apÄƒrea È™i fÄƒrÄƒ liniuÈ›Äƒ Ã®nainte
        name = re.sub(r'\s+[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?$', '', name)

        # Lista completÄƒ de sufixe de eliminat
        suffixes_to_remove = [
            'scan', 'ctrl', 'retail', r'cop\d+', 'Vp', 'draft', 'final',
            'ocr', 'OCR', 'edit', 'edited', 'rev', 'revised', 'proof',
            'beta', 'alpha', 'test', 'demo', 'sample', 'preview',
            'full', 'complete', 'fix', 'fixed', 'corrected'
        ]

        # ConstruieÈ™te pattern-ul pentru toate sufixele
        suffix_pattern = '|'.join(suffixes_to_remove)

        # EliminÄƒ sufixele care apar dupÄƒ ultima " - "
        parts = name.rsplit(' - ', 1)
        if len(parts) == 2:
            last_part = parts[1].strip()
            if re.match(f'^({suffix_pattern})$', last_part, re.IGNORECASE):
                name = parts[0]

        # EliminÄƒ numerele de la Ã®nceput din titlu (ex: "4. Comosicus" -> "Comosicus")
        if ' - ' in name:
            parts = name.split(' - ', 1)
            if len(parts) == 2:
                # CurÄƒÈ›Äƒ partea de titlu de numere de la Ã®nceput
                title = re.sub(r'^\d+\.\s*', '', parts[1])
                name = f"{parts[0]} - {title}"

        # CurÄƒÈ›Äƒ spaÈ›iile multiple È™i spaÈ›iile de la Ã®nceput/sfÃ¢rÈ™it
        name = re.sub(r'\s+', ' ', name).strip()

        # EliminÄƒ eventuale liniuÈ›e rÄƒmase la sfÃ¢rÈ™it
        name = re.sub(r'\s*[-â€“]\s*$', '', name)

        logger.debug(f"âœ¨ Titlu curÄƒÈ›at: {name}")

        return name

    def scan_folders(self) -> List[Dict[str, Any]]:
        """ScaneazÄƒ recursiv toate folderele È™i returneazÄƒ lista de fiÈ™iere de verificat"""
        tasks = []
        logger.info(f"ğŸ“‚ Scanez folderul: {ARCHIVE_PATH}")

        try:
            for root, dirs, files in os.walk(ARCHIVE_PATH):
                folder = Path(root)

                # GÄƒseÈ™te fiÈ™ierele relevante
                relevant_files = []
                for file in files:
                    if Path(file).suffix.lower() in RELEVANT_EXTENSIONS:
                        file_path = folder / file
                        relevant_files.append(file_path)

                if not relevant_files:
                    continue

                logger.debug(f"ğŸ“ Folder {folder.name}: {len(relevant_files)} fiÈ™iere relevante")

                # GrupeazÄƒ fiÈ™ierele dupÄƒ numele principal (fÄƒrÄƒ extensie È™i versiune)
                file_groups = {}
                for file_path in relevant_files:
                    clean_name = self.clean_title_for_search(file_path.name)
                    if clean_name not in file_groups:
                        file_groups[clean_name] = []
                    file_groups[clean_name].append(file_path)

                # AdaugÄƒ fiecare grup unic pentru verificare
                for clean_name, file_list in file_groups.items():
                    # CalculeazÄƒ dimensiunea totalÄƒ
                    total_size = sum(f.stat().st_size for f in file_list if f.exists())

                    tasks.append({
                        "search_title": clean_name,
                        "folder": folder,
                        "files": file_list,
                        "size": total_size
                    })

            logger.info(f"ğŸ“Š GÄƒsite {len(tasks)} titluri unice de verificat")
            return tasks

        except Exception as e:
            logger.error(f"âŒ Eroare la scanarea folderelor: {e}")
            return []

    def check_archive_api(self, title: str) -> bool:
        """VerificÄƒ dacÄƒ un titlu existÄƒ pe Internet Archive folosind API-ul JSON"""
        url = "https://archive.org/advancedsearch.php"

        # PregÄƒteÈ™te parametrii pentru API
        params = {
            "q": f'title:("{title}")',  # CÄƒutare exactÄƒ cu ghilimele
            "fl[]": ["identifier", "title"],  # ReturneazÄƒ ID È™i titlu
            "rows": 10,  # VerificÄƒ primele 10 rezultate
            "output": "json",
            "sort": "downloads desc"  # SorteazÄƒ dupÄƒ popularitate
        }

        logger.info(f"ğŸ” Caut pe Archive.org: '{title}'")
        logger.debug(f"   URL: {url}")
        logger.debug(f"   Parametri: {params}")

        for attempt in range(1, MAX_API_ATTEMPTS + 1):
            try:
                response = self.session.get(url, params=params, timeout=API_TIMEOUT)
                response.raise_for_status()

                data = response.json()
                response_data = data.get("response", {})
                num_found = response_data.get("numFound", 0)
                docs = response_data.get("docs", [])

                logger.debug(f"   ğŸ“Š Rezultate gÄƒsite: {num_found}")

                if num_found > 0:
                    # VerificÄƒ dacÄƒ vreun rezultat se potriveÈ™te
                    for doc in docs:
                        doc_title = doc.get("title", "")
                        doc_id = doc.get("identifier", "")

                        # CurÄƒÈ›Äƒ È™i titlul din rezultat pentru comparaÈ›ie
                        clean_doc_title = self.clean_title_for_search(doc_title)

                        # NormalizeazÄƒ pentru comparaÈ›ie
                        search_normalized = re.sub(r'[^a-z0-9\s]', '', title.lower())
                        search_normalized = re.sub(r'\s+', ' ', search_normalized).strip()

                        doc_normalized = re.sub(r'[^a-z0-9\s]', '', clean_doc_title.lower())
                        doc_normalized = re.sub(r'\s+', ' ', doc_normalized).strip()

                        # VerificÄƒ similaritatea
                        if self.are_titles_similar(search_normalized, doc_normalized):
                            logger.info(f"   âœ… GÄ‚SIT pe Archive.org!")
                            logger.info(f"      ID: {doc_id}")
                            logger.info(f"      Titlu: {doc_title}")
                            return True

                    logger.info(f"   âŒ Nu s-a gÄƒsit potrivire exactÄƒ dintre {num_found} rezultate")
                else:
                    logger.info(f"   â„¹ï¸ Niciun rezultat gÄƒsit")

                return False

            except requests.RequestException as e:
                logger.warning(f"   âš ï¸ Eroare API (Ã®ncercarea {attempt}/{MAX_API_ATTEMPTS}): {e}")
                if attempt < MAX_API_ATTEMPTS:
                    time.sleep(2 ** attempt)  # Backoff exponenÈ›ial
            except json.JSONDecodeError as e:
                logger.error(f"   âŒ Eroare la parsarea rÄƒspunsului JSON: {e}")
                return False

        logger.error(f"   âŒ EÈ™uat dupÄƒ {MAX_API_ATTEMPTS} Ã®ncercÄƒri")
        return False

    def are_titles_similar(self, title1: str, title2: str) -> bool:
        """VerificÄƒ dacÄƒ douÄƒ titluri sunt similare (tolerant la mici diferenÈ›e)"""
        # EliminÄƒ toate spaÈ›iile pentru comparaÈ›ie
        t1 = title1.replace(" ", "")
        t2 = title2.replace(" ", "")

        # VerificÄƒ egalitate exactÄƒ
        if t1 == t2:
            return True

        # VerificÄƒ dacÄƒ unul este conÈ›inut Ã®n celÄƒlalt
        if len(t1) > 3 and len(t2) > 3:  # Minim 4 caractere
            if t1 in t2 or t2 in t1:
                return True

        # VerificÄƒ dacÄƒ toate cuvintele din titlul cÄƒutat sunt Ã®n rezultat
        words1 = set(title1.split())
        words2 = set(title2.split())

        # EliminÄƒ cuvintele scurte (articole, prepoziÈ›ii)
        words1 = {w for w in words1 if len(w) > 2}
        words2 = {w for w in words2 if len(w) > 2}

        if words1 and words2:
            # VerificÄƒ dacÄƒ cel puÈ›in 80% din cuvinte se potrivesc
            common_words = words1.intersection(words2)
            if len(common_words) / len(words1) >= 0.8:
                return True

        return False

    def delete_folder(self, task: Dict[str, Any]):
        """È˜terge un folder È™i toate subfiÈ™ierele sale"""
        folder = task['folder']

        # DeterminÄƒ ce folder sÄƒ È™tergem
        if folder.parent != ARCHIVE_PATH:
            folder_to_delete = folder.parent
        else:
            folder_to_delete = folder

        try:
            logger.warning(f"ğŸ—‘ï¸ È˜terg folderul: {folder_to_delete}")
            logger.info(f"   Motivul: FiÈ™ierul '{task['search_title']}' existÄƒ pe Archive.org")

            # CalculeazÄƒ spaÈ›iul eliberat (Ã®n MB)
            size_mb = task.get('size', 0) / (1024 * 1024)

            # CreeazÄƒ backup Ã®n state Ã®nainte de È™tergere
            backup_info = {
                "folder": str(folder_to_delete),
                "title": task['search_title'],
                "files": [str(f) for f in task['files']],
                "size_mb": round(size_mb, 2),
                "deleted_at": datetime.now().isoformat()
            }

            # È˜terge folderul
            shutil.rmtree(folder_to_delete)

            # AsigurÄƒ cÄƒ deleted_folders existÄƒ Ã®nainte de a adÄƒuga
            if "deleted_folders" not in self.state:
                self.state["deleted_folders"] = []

            # ActualizeazÄƒ statisticile
            self.state["deleted_folders"].append(backup_info)
            self.deleted_count += 1

            # AsigurÄƒ cÄƒ stats existÄƒ
            if "stats" not in self.state:
                self.state["stats"] = {
                    "total_checked": 0,
                    "total_deleted": 0,
                    "total_space_saved_mb": 0.0
                }

            self.state["stats"]["total_deleted"] += 1
            self.state["stats"]["total_space_saved_mb"] = round(
                self.state["stats"]["total_space_saved_mb"] + size_mb, 2
            )

            logger.info(f"   âœ… Folder È™ters cu succes! ({size_mb:.2f} MB eliberat)")

            # SalveazÄƒ starea imediat dupÄƒ È™tergere
            self.save_state()

            return True

        except Exception as e:
            logger.error(f"   âŒ Eroare la È™tergerea folderului: {e}")
            self.error_count += 1
            return False

    def reset_processed_folders(self):
        """ReseteazÄƒ lista de foldere procesate pentru a le verifica din nou"""
        logger.info("ğŸ”„ Resetez lista de foldere procesate pentru reverificare...")
        self.state["processed_folders"] = []
        self.save_state()

    def generate_report(self):
        """GenereazÄƒ un raport final detaliat"""
        report_lines = [
            "\n" + "=" * 60,
            "ğŸ“Š RAPORT FINAL",
            "=" * 60,
            f"âœ… FiÈ™iere verificate: {self.checked_count}",
            f"ğŸ—‘ï¸ Foldere È™terse: {self.deleted_count}",
            f"âŒ Erori Ã®ntÃ¢mpinate: {self.error_count}",
            f"ğŸ’¾ SpaÈ›iu total eliberat: {self.state['stats']['total_space_saved_mb']:.2f} MB",
            f"ğŸ“ˆ Total istoric foldere È™terse: {self.state['stats']['total_deleted']}",
            "=" * 60
        ]

        for line in report_lines:
            logger.info(line)

        if self.state["deleted_folders"]:
            logger.info("\nğŸ“‹ FOLDERE È˜TERSE ÃN ACEASTÄ‚ SESIUNE:")
            for i, folder_info in enumerate(self.state["deleted_folders"][-self.deleted_count:], 1):
                logger.info(f"   {i}. {folder_info['folder']}")
                logger.info(f"      Titlu: {folder_info['title']}")
                logger.info(f"      SpaÈ›iu eliberat: {folder_info.get('size_mb', 0):.2f} MB")

    def run(self):
        """RuleazÄƒ procesul principal"""
        logger.info("=" * 60)
        logger.info("ğŸš€ START - Internet Archive Duplicate Checker")
        logger.info(f"ğŸ“ Folder verificat: {ARCHIVE_PATH}")
        logger.info(f"â±ï¸ Delay Ã®ntre cÄƒutÄƒri: {DELAY_BETWEEN_SEARCHES} secunde")
        logger.info("=" * 60)

        # VerificÄƒ dacÄƒ folderul existÄƒ
        if not ARCHIVE_PATH.exists():
            logger.error(f"âŒ Folderul {ARCHIVE_PATH} nu existÄƒ!")
            return False

        # ReseteazÄƒ folderele procesate pentru a le verifica din nou
        self.reset_processed_folders()

        try:
            # ScaneazÄƒ folderele
            tasks = self.scan_folders()

            if not tasks:
                logger.info("âœ… Nu sunt fiÈ™iere de verificat!")
                return True

            # ProceseazÄƒ fiecare task
            for i, task in enumerate(tasks, 1):
                folder_str = str(task["folder"])

                # Sari peste folderele deja procesate Ã®n aceastÄƒ sesiune
                if folder_str in self.state["processed_folders"]:
                    logger.debug(f"â­ï¸ Folder deja procesat: {folder_str}")
                    continue

                logger.info(f"\nğŸ“Š Progres: {i}/{len(tasks)}")
                logger.info(f"ğŸ“‚ Folder: {task['folder'].name}")
                logger.info(f"ğŸ“„ Titlu cÄƒutat: {task['search_title']}")

                # VerificÄƒ pe Archive.org
                found = self.check_archive_api(task['search_title'])

                if found:
                    # È˜terge folderul
                    self.delete_folder(task)
                else:
                    logger.info(f"   â„¹ï¸ PÄƒstrez folderul - nu existÄƒ pe Archive.org")

                # MarcheazÄƒ ca procesat
                self.state["processed_folders"].append(folder_str)
                self.checked_count += 1
                self.state["stats"]["total_checked"] += 1

                # SalveazÄƒ starea periodic (la fiecare 5 foldere)
                if i % 5 == 0:
                    self.save_state()

                # AÈ™teaptÄƒ Ã®ntre cÄƒutÄƒri (exceptÃ¢nd ultima)
                if i < len(tasks):
                    logger.info(f"â³ AÈ™tept {DELAY_BETWEEN_SEARCHES} secunde...")
                    time.sleep(DELAY_BETWEEN_SEARCHES)

            # SalveazÄƒ starea finalÄƒ
            self.save_state()

            # GenereazÄƒ raportul final
            self.generate_report()

            return True

        except KeyboardInterrupt:
            logger.warning("\nâš ï¸ Proces Ã®ntrerupt de utilizator")
            self.save_state()
            self.generate_report()
            return False

        except Exception as e:
            logger.error(f"\nâŒ Eroare fatalÄƒ: {e}", exc_info=True)
            self.save_state()
            return False


def main():
    """FuncÈ›ia principalÄƒ"""
    try:
        checker = ArchiveDuplicateChecker()
        success = checker.run()

        if success:
            print("\nâœ… Proces finalizat cu succes!")
        else:
            print("\nâŒ Proces finalizat cu erori!")

    except Exception as e:
        print(f"âŒ Eroare fatalÄƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()