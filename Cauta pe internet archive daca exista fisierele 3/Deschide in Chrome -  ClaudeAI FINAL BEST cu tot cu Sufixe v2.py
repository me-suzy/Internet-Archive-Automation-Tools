#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Internet Archive Duplicate Checker - Hybrid Fast Method
CombineazÄƒ cÄƒutarea rapidÄƒ pe archive.org/search cu simularea Chrome upload.

Strategia hibridÄƒ:
1. Nivel 1: CÄƒutare rapidÄƒ pe archive.org/search - analizeazÄƒ HTML pentru duplicate
2. Nivel 2: Simulare Chrome upload - doar pentru cazurile incerte

Ãnainte de rulare:
1. RuleazÄƒ start_chrome_debug.bat pentru a porni Chrome Ã®n debug mode
2. RuleazÄƒ acest script
"""

import os
import re
import time
import shutil
import json
import logging
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple
from urllib.parse import quote_plus
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import WebDriverException, TimeoutException, NoSuchElementException

# ============= CONFIGURÄ‚RI =============
ARCHIVE_PATH = Path(r"g:\ARHIVA\B+")
ARCHIVE_URL = "https://archive.org/upload"
SEARCH_BASE_URL = "https://archive.org/search"
STATE_FILE = Path("archive_duplicate_hybrid_state.json")
LOG_FILE = Path(f"archive_duplicate_hybrid_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

# Extensii prioritare pentru verificare (Ã®n ordinea prioritÄƒÈ›ii)
PRIORITY_EXTENSIONS = ['.pdf', '.epub', '.mobi', '.djvu', '.docx', '.doc', '.lit', '.rtf']

# Extensii de ignorat
IGNORE_EXTENSIONS = ['.jpg', '.png']

# Configurare requests
MAX_RETRIES = 3
REQUEST_TIMEOUT = 15

# ============= CONFIGURARE LOGGER =============
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ArchiveHybridChecker")


class ArchiveHybridChecker:
    def __init__(self, timeout=30):
        self.timeout = timeout
        self.driver = None
        self.wait = None
        self.state = self.load_state()
        self.deleted_count = 0
        self.checked_count = 0
        self.error_count = 0
        self.base_window = None
        self.chrome_needed = False

        # Statistici metode
        self.fast_search_hits = 0
        self.chrome_fallback_used = 0

        # Session pentru requests
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

    def load_state(self) -> Dict[str, Any]:
        """ÃncarcÄƒ starea salvatÄƒ anterior"""
        default_state = {
            "processed_folders": [],
            "deleted_folders": [],
            "last_processed": None,
            "stats": {
                "total_checked": 0,
                "total_deleted": 0,
                "total_space_saved_mb": 0.0,
                "fast_search_hits": 0,
                "chrome_fallback_used": 0
            }
        }

        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    state = json.load(f)

                    # Compatibilitate cu versiuni anterioare
                    if "deleted_folders" not in state:
                        state["deleted_folders"] = []
                    if "stats" not in state:
                        state["stats"] = default_state["stats"]
                    if "processed_folders" not in state:
                        state["processed_folders"] = []

                    logger.info(f"ğŸ“‹ Stare Ã®ncÄƒrcatÄƒ: {len(state.get('processed_folders', []))} foldere procesate anterior")
                    return state
            except Exception as e:
                logger.warning(f"âš ï¸ Nu s-a putut Ã®ncÄƒrca starea: {e}")

        return default_state

    def save_state(self):
        """SalveazÄƒ starea curentÄƒ"""
        self.state["last_processed"] = datetime.now().isoformat()
        self.state["stats"]["fast_search_hits"] = self.fast_search_hits
        self.state["stats"]["chrome_fallback_used"] = self.chrome_fallback_used

        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)
            logger.debug("ğŸ’¾ Stare salvatÄƒ cu succes")
        except Exception as e:
            logger.error(f"âŒ Eroare la salvarea stÄƒrii: {e}")

    def setup_chrome_driver(self):
        """ConecteazÄƒ la instanÈ›a Chrome cu debugging activatÄƒ (doar cÃ¢nd e necesar)"""
        if self.driver:
            return True  # Deja conectat

        try:
            logger.info("ğŸ”§ Conectare la Chrome debug mode...")
            chrome_options = Options()
            chrome_options.add_experimental_option("debuggerAddress", "127.0.0.1:9222")

            # ConfigurÄƒri pentru upload
            prefs = {
                "download.default_directory": os.path.abspath(os.getcwd()),
                "download.prompt_for_download": False,
                "download.directory_upgrade": True,
                "safebrowsing.enabled": True,
            }
            chrome_options.add_experimental_option("prefs", prefs)

            self.driver = webdriver.Chrome(options=chrome_options)
            self.wait = WebDriverWait(self.driver, self.timeout)

            # PÄƒstreazÄƒ referinÈ›a la primul tab (tab-ul de bazÄƒ)
            self.base_window = self.driver.current_window_handle

            logger.info("âœ… Conectat la Chrome cu succes")
            logger.info(f"ğŸ  Tab de bazÄƒ: {self.base_window}")
            return True

        except WebDriverException as e:
            logger.error(f"âŒ Eroare la conectarea la Chrome: {e}")
            logger.error("ğŸ’¡ AsigurÄƒ-te cÄƒ ai rulat start_chrome_debug.bat Ã®nainte!")
            return False

    def clean_title_for_search(self, filename: str) -> str:
        """CurÄƒÈ›Äƒ titlul pentru cÄƒutare pe Archive.org"""
        name = Path(filename).stem
        logger.debug(f"ğŸ”§ CurÄƒÈ›are titlu original: {filename}")

        # EliminÄƒ sufixele de data
        name = re.sub(r'[_-]\d{6,8}$', '', name)

        # EliminÄƒ parantezele È™i conÈ›inutul lor
        name = re.sub(r'\s*\([^)]*\)', '', name)
        name = re.sub(r'\s*\[[^\]]*\]', '', name)

        # EliminÄƒ versiunile
        name = re.sub(r'\s*[-â€“]\s*[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?(?:\s*[-â€“]\s*\d+)?$', '', name)
        name = re.sub(r'\s+[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?$', '', name)

        # EliminÄƒ sufixele tehnice
        suffixes = ['scan', 'ctrl', 'retail', r'cop\d+', 'Vp', 'draft', 'final',
                   'ocr', 'OCR', 'edit', 'edited', 'rev', 'revised', 'proof',
                   'beta', 'alpha', 'test', 'demo', 'sample', 'preview',
                   'full', 'complete', 'fix', 'fixed', 'corrected']

        # EliminÄƒ sufixele care apar dupÄƒ ultima " - "
        parts = name.rsplit(' - ', 1)
        if len(parts) == 2:
            last_part = parts[1].strip()
            suffix_pattern = '|'.join(suffixes)
            if re.match(f'^({suffix_pattern})$', last_part, re.IGNORECASE):
                name = parts[0]

        # EliminÄƒ #TAGS (ex: #ISTOR)
        name = re.sub(r'\s*#\w+', '', name)

        # CurÄƒÈ›Äƒ spaÈ›iile
        name = re.sub(r'\s+', ' ', name).strip()
        name = re.sub(r'\s*[-â€“]\s*$', '', name)

        logger.debug(f"âœ¨ Titlu curÄƒÈ›at: {name}")
        return name

    def scan_folders(self) -> List[Dict[str, Any]]:
        """ScaneazÄƒ folderele È™i returneazÄƒ lista de foldere de verificat"""
        folders_to_check = []
        logger.info(f"ğŸ“‚ Scanez folderul: {ARCHIVE_PATH}")

        try:
            for folder_path in ARCHIVE_PATH.iterdir():
                if not folder_path.is_dir():
                    continue

                # GÄƒseÈ™te fiÈ™ierele relevante recursiv
                relevant_files = []
                for root, dirs, files in os.walk(folder_path):
                    for file in files:
                        file_path = Path(root) / file
                        if file_path.suffix.lower() in PRIORITY_EXTENSIONS:
                            relevant_files.append(file_path)

                if not relevant_files:
                    logger.debug(f"ğŸ“ {folder_path.name}: Nu are fiÈ™iere relevante")
                    continue

                # GÄƒseÈ™te primul fiÈ™ier cu prioritatea cea mai mare
                priority_file = self.find_priority_file(relevant_files)
                if not priority_file:
                    logger.debug(f"ğŸ“ {folder_path.name}: Nu are fiÈ™iere cu extensii prioritare")
                    continue

                # CalculeazÄƒ dimensiunea totalÄƒ
                total_size = sum(f.stat().st_size for f in relevant_files if f.exists())

                # GenereazÄƒ titlul pentru cÄƒutare
                search_title = self.clean_title_for_search(priority_file.name)

                folders_to_check.append({
                    "folder_path": folder_path,
                    "priority_file": priority_file,
                    "all_files": relevant_files,
                    "total_size": total_size,
                    "search_title": search_title
                })

                logger.debug(f"ğŸ“ {folder_path.name}: {len(relevant_files)} fiÈ™iere, titlu cÄƒutare: {search_title}")

            logger.info(f"ğŸ“Š GÄƒsite {len(folders_to_check)} foldere de verificat")
            return folders_to_check

        except Exception as e:
            logger.error(f"âŒ Eroare la scanarea folderelor: {e}")
            return []

    def find_priority_file(self, files: List[Path]) -> Path:
        """GÄƒseÈ™te primul fiÈ™ier conform prioritÄƒÈ›ii"""
        for ext in PRIORITY_EXTENSIONS:
            for file in files:
                if file.suffix.lower() == ext:
                    return file
        return None

    def search_archive_org_web(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """
        CÄƒutare rapidÄƒ pe archive.org/search pentru a detecta duplicate
        Returns: (is_duplicate, method_used)
        """
        search_title = folder_info["search_title"]
        folder_path = folder_info["folder_path"]

        logger.info(f"ğŸ” NIVEL 1 - CÄƒutare rapidÄƒ pentru: {search_title}")

        # ConstruieÈ™te URL-ul de cÄƒutare
        query = quote_plus(search_title)
        search_url = f"{SEARCH_BASE_URL}?query={query}"

        logger.debug(f"ğŸŒ URL cÄƒutare: {search_url}")

        for attempt in range(MAX_RETRIES):
            try:
                response = self.session.get(search_url, timeout=REQUEST_TIMEOUT)
                response.raise_for_status()

                # Parse HTML cu BeautifulSoup
                soup = BeautifulSoup(response.content, 'html.parser')

                # GÄƒseÈ™te toate elementele <h4> cu clasa "truncated"
                title_elements = soup.find_all('h4', class_='truncated')

                if not title_elements:
                    logger.info(f"   â„¹ï¸ Niciun rezultat gÄƒsit Ã®n cÄƒutare")
                    return False, "no_results"

                # Extrage titlurile È™i verificÄƒ duplicate
                found_titles = []
                for h4 in title_elements:
                    title_attr = h4.get('title', '').strip()
                    title_text = h4.get_text().strip()

                    # FoloseÈ™te titlul din attribute sau text
                    final_title = title_attr if title_attr else title_text
                    if final_title:
                        found_titles.append(final_title)

                logger.info(f"   ğŸ“Š GÄƒsite {len(found_titles)} rezultate:")
                for i, title in enumerate(found_titles[:5], 1):  # AfiÈ™eazÄƒ primele 5
                    logger.info(f"      {i}. {title}")

                # VerificÄƒ duplicate - cÄƒutÄƒ titluri identice
                if len(found_titles) >= 2:
                    # VerificÄƒ dacÄƒ sunt cel puÈ›in 2 titluri identice
                    title_counts = {}
                    for title in found_titles:
                        # NormalizeazÄƒ titlul pentru comparaÈ›ie
                        normalized = re.sub(r'[^\w\s]', ' ', title.lower())
                        normalized = re.sub(r'\s+', ' ', normalized).strip()

                        title_counts[normalized] = title_counts.get(normalized, 0) + 1

                    # VerificÄƒ dacÄƒ vreun titlu apare de mai multe ori
                    for title, count in title_counts.items():
                        if count >= 2:
                            logger.info(f"   ğŸ¯ DUPLICAT GÄ‚SIT prin cÄƒutare rapidÄƒ!")
                            logger.info(f"      Titlu duplicat: {title} (apare de {count} ori)")
                            return True, "fast_search"

                logger.info(f"   âœ… Un singur rezultat gÄƒsit - nu sunt duplicate evidente")
                return False, "single_result"

            except requests.RequestException as e:
                logger.warning(f"   âš ï¸ Eroare la cÄƒutarea web (Ã®ncercarea {attempt + 1}): {e}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(2 ** attempt)
                    continue
                else:
                    logger.error(f"   âŒ CÄƒutare web eÈ™uatÄƒ dupÄƒ {MAX_RETRIES} Ã®ncercÄƒri")
                    return False, "search_error"

            except Exception as e:
                logger.error(f"   âŒ Eroare neaÈ™teptatÄƒ la cÄƒutarea web: {e}")
                return False, "search_error"

    def check_folder_with_chrome(self, folder_info: Dict[str, Any]) -> bool:
        """NIVEL 2: VerificÄƒ prin simularea upload-ului Chrome (fallback)"""
        folder_path = folder_info["folder_path"]
        priority_file = folder_info["priority_file"]

        logger.info(f"ğŸ”§ NIVEL 2 - Chrome simulation pentru: {folder_path.name}")

        # AsigurÄƒ-te cÄƒ Chrome este conectat
        if not self.setup_chrome_driver():
            logger.error("âŒ Nu pot conecta la Chrome pentru fallback")
            return False

        try:
            # Deschide un nou tab pentru upload
            logger.info("ğŸŒ Deschid tab nou pentru archive.org/upload...")
            self.driver.execute_script("window.open('');")
            new_window = self.driver.window_handles[-1]
            self.driver.switch_to.window(new_window)

            # NavigheazÄƒ la pagina de upload
            self.driver.get(ARCHIVE_URL)

            # AÈ™teaptÄƒ ca pagina sÄƒ se Ã®ncarce
            self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'input[type="file"]')))
            logger.info("âœ… Pagina de upload Ã®ncÄƒrcatÄƒ")

            # GÄƒseÈ™te input-ul pentru fiÈ™iere
            file_input = self.wait.until(EC.presence_of_element_located((By.CSS_SELECTOR, 'input[type="file"]')))

            # ÃncarcÄƒ fiÈ™ierul
            logger.info(f"ğŸ“¤ ÃncarcÄƒ fiÈ™ierul: {priority_file.name}")
            file_input.send_keys(str(priority_file.absolute()))

            # AÈ™teaptÄƒ ca fiÈ™ierul sÄƒ se proceseze È™i sÄƒ se genereze Page URL
            logger.info("â³ AÈ™tept 8 secunde pentru generarea Page URL...")
            time.sleep(8)

            # VerificÄƒ Page URL pentru sufixe duplicate
            is_duplicate = self.check_page_url_for_suffix()

            # Ãnchide tab-ul
            self.close_current_tab_and_return_to_base()

            if is_duplicate:
                logger.info("ğŸ¯ DUPLICAT CONFIRMAT prin Chrome simulation!")
                return True
            else:
                logger.info("âœ… Nu e duplicat conform Chrome simulation")
                return False

        except Exception as e:
            logger.error(f"âŒ Eroare la Chrome simulation: {e}")

            # ÃncearcÄƒ sÄƒ Ã®nchidÄƒ tab-ul chiar È™i Ã®n caz de eroare
            try:
                self.close_current_tab_and_return_to_base()
            except:
                pass

            return False

    def check_page_url_for_suffix(self) -> bool:
        """VerificÄƒ dacÄƒ Page URL-ul conÈ›ine sufixe duplicate"""
        try:
            # GÄƒseÈ™te elementul cu ID-ul item_id care conÈ›ine identifier-ul
            item_id_element = self.wait.until(
                EC.presence_of_element_located((By.ID, "item_id"))
            )

            page_identifier = item_id_element.text.strip() or item_id_element.get_attribute("title") or ""

            if not page_identifier:
                logger.warning("âš ï¸ Nu am putut extrage Page URL identifier")
                return False

            logger.info(f"ğŸ“‹ Page URL identifier: {page_identifier}")

            # VerificÄƒ sufixele duplicate
            if re.search(r'_\d{6}$|_\d{8}$', page_identifier):
                logger.info(f"ğŸš« GÄ‚SIT SUFIX DUPLICAT Ã®n identifier: {page_identifier}")
                return True
            else:
                logger.info(f"âœ… Identifier OK (fÄƒrÄƒ sufix duplicat): {page_identifier}")
                return False

        except TimeoutException:
            logger.warning("âš ï¸ Timeout la aÈ™teptarea Page URL - probabil nu s-a generat Ã®ncÄƒ")
            return False
        except Exception as e:
            logger.error(f"âŒ Eroare la verificarea Page URL: {e}")
            return False

    def close_current_tab_and_return_to_base(self):
        """Ãnchide tab-ul curent È™i revine la tab-ul de bazÄƒ"""
        try:
            current_window = self.driver.current_window_handle

            if current_window != self.base_window:
                logger.info("ğŸ—‚ï¸ Ãnchid tab-ul curent...")
                self.driver.close()

                # Revine la tab-ul de bazÄƒ
                self.driver.switch_to.window(self.base_window)
                logger.info("ğŸ  Revenit la tab-ul de bazÄƒ")
            else:
                logger.warning("âš ï¸ Sunt deja pe tab-ul de bazÄƒ")

        except Exception as e:
            logger.error(f"âŒ Eroare la Ã®nchiderea tab-ului: {e}")
            # ÃncearcÄƒ sÄƒ revinÄƒ la primul tab disponibil
            try:
                if self.driver and self.driver.window_handles:
                    self.driver.switch_to.window(self.driver.window_handles[0])
                    self.base_window = self.driver.current_window_handle
            except:
                pass

    def check_folder_hybrid(self, folder_info: Dict[str, Any]) -> bool:
        """VerificÄƒ un folder folosind strategia hibridÄƒ (fast search + Chrome fallback)"""
        folder_path = folder_info["folder_path"]

        logger.info(f"\nğŸ“‚ Verific folderul: {folder_path.name}")
        logger.info(f"ğŸ“„ Titlu cÄƒutare: {folder_info['search_title']}")

        # Sari peste folderele deja procesate
        folder_str = str(folder_path)
        if folder_str in self.state["processed_folders"]:
            logger.info(f"â­ï¸ Folder deja procesat, sar peste")
            return False

        try:
            # NIVEL 1: CÄƒutare rapidÄƒ pe archive.org/search
            is_duplicate, method = self.search_archive_org_web(folder_info)

            if is_duplicate and method == "fast_search":
                logger.info("ğŸ¯ DUPLICAT DETECTAT prin cÄƒutare rapidÄƒ!")
                self.fast_search_hits += 1
                self.delete_folder(folder_info, "Duplicat gÄƒsit prin cÄƒutare rapidÄƒ")
                return True

            elif method in ["no_results", "single_result"]:
                # NIVEL 2: Chrome simulation pentru confirmare
                logger.info("ğŸ”§ Trec la Chrome simulation pentru confirmare...")
                self.chrome_fallback_used += 1

                is_duplicate_chrome = self.check_folder_with_chrome(folder_info)

                if is_duplicate_chrome:
                    logger.info("ğŸ¯ DUPLICAT CONFIRMAT prin Chrome simulation!")
                    self.delete_folder(folder_info, "Duplicat confirmat prin Chrome simulation")
                    return True
                else:
                    logger.info("âœ… NU e duplicat - pÄƒstrez folderul")
                    # MarcheazÄƒ ca procesat
                    self.state["processed_folders"].append(folder_str)
                    self.checked_count += 1
                    self.state["stats"]["total_checked"] += 1
                    return False

            elif method == "search_error":
                logger.warning("âš ï¸ Eroare la cÄƒutarea web - Ã®ncerc Chrome simulation...")
                self.chrome_fallback_used += 1

                is_duplicate_chrome = self.check_folder_with_chrome(folder_info)

                if is_duplicate_chrome:
                    self.delete_folder(folder_info, "Duplicat gÄƒsit prin Chrome simulation (dupÄƒ eroare search)")
                    return True
                else:
                    # MarcheazÄƒ ca procesat
                    self.state["processed_folders"].append(folder_str)
                    self.checked_count += 1
                    self.state["stats"]["total_checked"] += 1
                    return False

            else:
                logger.warning(f"âš ï¸ Rezultat neaÈ™teptat: {method}")
                return False

        except Exception as e:
            logger.error(f"âŒ Eroare la verificarea folderului {folder_path.name}: {e}")
            self.error_count += 1
            return False

    def delete_folder(self, folder_info: Dict[str, Any], reason: str):
        """È˜terge folderul cu duplicat"""
        folder_path = folder_info["folder_path"]

        try:
            logger.warning(f"ğŸ—‘ï¸ È˜terg folderul duplicat: {folder_path}")
            logger.info(f"   Motiv: {reason}")

            # CalculeazÄƒ spaÈ›iul eliberat
            size_mb = folder_info.get('total_size', 0) / (1024 * 1024)

            # CreeazÄƒ backup info
            backup_info = {
                "folder": str(folder_path),
                "priority_file": str(folder_info["priority_file"]),
                "search_title": folder_info["search_title"],
                "all_files": [str(f) for f in folder_info["all_files"]],
                "size_mb": round(size_mb, 2),
                "deleted_at": datetime.now().isoformat(),
                "reason": reason
            }

            # È˜terge folderul
            shutil.rmtree(folder_path)

            # ActualizeazÄƒ statisticile
            if "deleted_folders" not in self.state:
                self.state["deleted_folders"] = []

            self.state["deleted_folders"].append(backup_info)
            self.deleted_count += 1

            if "stats" not in self.state:
                self.state["stats"] = {"total_checked": 0, "total_deleted": 0, "total_space_saved_mb": 0.0}

            self.state["stats"]["total_deleted"] += 1
            self.state["stats"]["total_space_saved_mb"] = round(
                self.state["stats"]["total_space_saved_mb"] + size_mb, 2
            )

            logger.info(f"âœ… Folder È™ters cu succes! ({size_mb:.2f} MB eliberat)")

            # SalveazÄƒ starea imediat
            self.save_state()

        except Exception as e:
            logger.error(f"âŒ Eroare la È™tergerea folderului: {e}")
            self.error_count += 1

    def generate_report(self):
        """GenereazÄƒ raportul final"""
        report_lines = [
            "\n" + "=" * 60,
            "ğŸ“Š RAPORT FINAL - Hybrid Archive Checker",
            "=" * 60,
            f"âœ… Foldere verificate: {self.checked_count}",
            f"ğŸ—‘ï¸ Foldere È™terse (duplicate): {self.deleted_count}",
            f"âŒ Erori Ã®ntÃ¢mpinate: {self.error_count}",
            f"ğŸ’¾ SpaÈ›iu eliberat Ã®n aceastÄƒ sesiune: {sum(f.get('size_mb', 0) for f in self.state['deleted_folders'][-self.deleted_count:]):.2f} MB",
            "",
            "ğŸ“ˆ STATISTICI METODE:",
            f"âš¡ CÄƒutÄƒri rapide cu succes: {self.fast_search_hits}",
            f"ğŸ”§ Chrome simulation folosit: {self.chrome_fallback_used}",
            f"ğŸ“Š EficienÈ›Äƒ cÄƒutare rapidÄƒ: {(self.fast_search_hits / max(self.checked_count + self.deleted_count, 1)) * 100:.1f}%",
            "",
            f"ğŸ“ˆ STATISTICI TOTALE:",
            f"ğŸ“ˆ Total istoric foldere È™terse: {self.state['stats']['total_deleted']}",
            f"ğŸ“ˆ Total spaÈ›iu istoric eliberat: {self.state['stats']['total_space_saved_mb']:.2f} MB",
            "=" * 60
        ]

        for line in report_lines:
            logger.info(line)

        if self.deleted_count > 0:
            logger.info(f"\nğŸ“‹ FOLDERE È˜TERSE ÃN ACEASTÄ‚ SESIUNE:")
            recent_deleted = self.state["deleted_folders"][-self.deleted_count:]
            for i, folder_info in enumerate(recent_deleted, 1):
                logger.info(f"   {i}. {Path(folder_info['folder']).name}")
                logger.info(f"      Titlu: {folder_info.get('search_title', 'N/A')}")
                logger.info(f"      Motiv: {folder_info.get('reason', 'N/A')}")
                logger.info(f"      SpaÈ›iu eliberat: {folder_info.get('size_mb', 0):.2f} MB")

    def run(self):
        """RuleazÄƒ procesul principal"""
        logger.info("=" * 60)
        logger.info("ğŸš€ START - Archive.org Hybrid Duplicate Checker")
        logger.info(f"ğŸ“ Folder verificat: {ARCHIVE_PATH}")
        logger.info("âš¡ Strategia hibridÄƒ: CÄƒutare rapidÄƒ + Chrome fallback")
        logger.info("=" * 60)

        # VerificÄƒ folderul sursÄƒ
        if not ARCHIVE_PATH.exists():
            logger.error(f"âŒ Folderul {ARCHIVE_PATH} nu existÄƒ!")
            return False

        try:
            # ScaneazÄƒ folderele
            folders_to_check = self.scan_folders()

            if not folders_to_check:
                logger.info("âœ… Nu sunt foldere de verificat!")
                return True

            # ProceseazÄƒ fiecare folder
            for i, folder_info in enumerate(folders_to_check, 1):
                logger.info(f"\nğŸ“Š Progres: {i}/{len(folders_to_check)}")

                try:
                    self.check_folder_hybrid(folder_info)

                    # PauzÄƒ Ã®ntre verificÄƒri
                    if i < len(folders_to_check):
                        logger.info("â³ PauzÄƒ 2 secunde...")
                        time.sleep(2)

                except KeyboardInterrupt:
                    logger.warning("\nâš ï¸ Ãntrerupt de utilizator")
                    break
                except Exception as e:
                    logger.error(f"âŒ Eroare la procesarea folderului: {e}")
                    continue

            # SalveazÄƒ starea finalÄƒ
            self.save_state()

            # GenereazÄƒ raportul
            self.generate_report()

            return True

        except KeyboardInterrupt:
            logger.warning("\nâš ï¸ Proces Ã®ntrerupt manual")
            self.save_state()
            self.generate_report()
            return False
        except Exception as e:
            logger.error(f"\nâŒ Eroare fatalÄƒ: {e}", exc_info=True)
            self.save_state()
            return False
        finally:
            # Cleanup Chrome
            if self.driver:
                try:
                    # Ãnchide toate tab-urile deschise de script
                    current_windows = self.driver.window_handles
                    for window in current_windows:
                        if window != self.base_window:
                            try:
                                self.driver.switch_to.window(window)
                                self.driver.close()
                            except:
                                pass

                    # Revine la tab-ul de bazÄƒ
                    if self.base_window in self.driver.window_handles:
                        self.driver.switch_to.window(self.base_window)

                    logger.info("ğŸ§¹ Chrome cleanup completat")
                except Exception as cleanup_error:
                    logger.warning(f"âš ï¸ Eroare la cleanup: {cleanup_error}")


def main():
    """FuncÈ›ia principalÄƒ"""
    try:
        checker = ArchiveHybridChecker()
        success = checker.run()

        if success:
            print("\nâœ… Proces finalizat cu succes!")
        else:
            print("\nâŒ Proces finalizat cu erori!")

    except Exception as e:
        print(f"âŒ Eroare fatalÄƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()