#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Internet Archive Duplicate Checker - Pure Web Method (No Chrome Required)
DetecteazÄƒ duplicate pe Archive.org folosind doar HTTP requests È™i parsing HTML.

Strategiile de detectare:
1. CÄƒutare multiplÄƒ pe archive.org/search cu variante de titluri
2. CÄƒutare directÄƒ pe pattern-uri de identifier-uri cu wildcard
3. Test direct de existenÈ›Äƒ URL pentru sufixe comune

AVANTAJE:
- Nu necesitÄƒ Chrome sau Selenium
- Mult mai rapid decÃ¢t simularea upload
- FuncÈ›ioneazÄƒ pe orice sistem
- Detectare inteligentÄƒ a duplicatelor
"""

import os
import re
import time
import shutil
import json
import logging
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import quote_plus
from bs4 import BeautifulSoup

# ============= CONFIGURÄ‚RI =============
ARCHIVE_PATH = Path(r"g:\ARHIVA\C")
SEARCH_BASE_URL = "https://archive.org/search"
DETAILS_BASE_URL = "https://archive.org/details"
API_BASE_URL = "https://archive.org/advancedsearch.php"

STATE_FILE = Path("archive_duplicate_pure_web_state.json")
LOG_FILE = Path(f"archive_duplicate_web_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

# Extensii prioritare pentru verificare (Ã®n ordinea prioritÄƒÈ›ii)
PRIORITY_EXTENSIONS = ['.pdf', '.epub', '.mobi', '.djvu', '.docx', '.doc', '.lit', '.rtf']

# Extensii de ignorat
IGNORE_EXTENSIONS = ['.jpg', '.png']

# Configurare requests
MAX_RETRIES = 3
REQUEST_TIMEOUT = 15
DELAY_BETWEEN_REQUESTS = 0.3   # Secunde Ã®ntre requests pentru a nu suprasolicita serverul

# Sufixe comune de duplicate de testat
COMMON_DUPLICATE_SUFFIXES = [
    '_202508', '_20250806', '_202507', '_20250705', '_202506', '_20250604',
    '_202505', '_20250503', '_202504', '_20250402', '_202503', '_20250301',
    '_202502', '_20250201', '_202501', '_20250101', '_202412', '_20241201'
]

# ============= CONFIGURARE LOGGER =============
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ArchivePureWebChecker")


class ArchivePureWebChecker:
    def __init__(self):
        self.state = self.load_state()
        self.deleted_count = 0
        self.checked_count = 0
        self.error_count = 0

        # Statistici metode
        self.search_hits = 0
        self.api_hits = 0
        self.url_test_hits = 0

        # Session pentru requests cu retry È™i headers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

        # Adapter cu retry strategy
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        retry_strategy = Retry(
            total=MAX_RETRIES,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def load_state(self) -> Dict[str, Any]:
        """ÃncarcÄƒ starea salvatÄƒ anterior"""
        default_state = {
            "processed_folders": [],
            "deleted_folders": [],
            "last_processed": None,
            "stats": {
                "total_checked": 0,
                "total_deleted": 0,
                "total_space_saved_mb": 0.0,
                "search_hits": 0,
                "api_hits": 0,
                "url_test_hits": 0
            }
        }

        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    state = json.load(f)

                    # Compatibilitate cu versiuni anterioare
                    for key in default_state:
                        if key not in state:
                            state[key] = default_state[key]

                    for key in default_state["stats"]:
                        if key not in state["stats"]:
                            state["stats"][key] = default_state["stats"][key]

                    logger.info(f"ğŸ“‹ Stare Ã®ncÄƒrcatÄƒ: {len(state.get('processed_folders', []))} foldere procesate anterior")
                    return state
            except Exception as e:
                logger.warning(f"âš ï¸ Nu s-a putut Ã®ncÄƒrca starea: {e}")

        return default_state

    def save_state(self):
        """SalveazÄƒ starea curentÄƒ"""
        self.state["last_processed"] = datetime.now().isoformat()
        self.state["stats"]["search_hits"] = self.search_hits
        self.state["stats"]["api_hits"] = self.api_hits
        self.state["stats"]["url_test_hits"] = self.url_test_hits

        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)
            logger.debug("ğŸ’¾ Stare salvatÄƒ cu succes")
        except Exception as e:
            logger.error(f"âŒ Eroare la salvarea stÄƒrii: {e}")

    def clean_title_for_search(self, filename: str) -> str:
        """CurÄƒÈ›Äƒ titlul pentru cÄƒutare SIMPLÄ‚ pe Archive.org"""
        name = Path(filename).stem
        logger.debug(f"ğŸ”§ CurÄƒÈ›are titlu original: {filename}")

        # âš¡ PRIMUL: EliminÄƒ COMPLET parantezele È™i conÈ›inutul lor
        name = re.sub(r'\s*\([^)]*\)', '', name)  # (trad.) dispare complet
        name = re.sub(r'\s*\[[^\]]*\]', '', name)  # [ceva] dispare complet

        # âš¡ AL DOILEA: EliminÄƒ sufixele tehnice de la sfÃ¢rÈ™it
        # retail, scan, ctrl, ocr, vp, draft, final
        technical_suffixes = ['retail', 'scan', 'ctrl', 'ocr', 'vp', 'draft', 'final', 'edit', 'copy', 'backup']

        # EliminÄƒ dupÄƒ ultima " - " dacÄƒ este sufix tehnic
        parts = name.rsplit(' - ', 1)
        if len(parts) == 2:
            last_part = parts[1].strip().lower()
            if last_part in technical_suffixes:
                name = parts[0]  # PÄƒstreazÄƒ doar partea din faÈ›Äƒ

        # âš¡ AL TREILEA: EliminÄƒ toate punctuaÈ›iile rÄƒmase
        # PÄƒstreazÄƒ doar literele, cifrele È™i spaÈ›iile
        name = re.sub(r'[^\w\s]', ' ', name)

        # âš¡ AL PATRULEA: EliminÄƒ versiunile (v1, v.1.0, etc.)
        words = name.split()
        words = [w for w in words if not re.match(r'^v\.?\d', w.lower())]

        # CurÄƒÈ›Äƒ spaÈ›iile multiple
        result = ' '.join(words)
        result = re.sub(r'\s+', ' ', result).strip()

        logger.debug(f"âœ¨ Titlu pentru cÄƒutare simplÄƒ: '{result}'")
        return result

    def generate_identifier_base(self, title: str) -> str:
        """GenereazÄƒ base identifier-ul aÈ™a cum l-ar genera Archive.org"""
        # Ãncepe cu titlul
        identifier = title.lower()

        # ÃnlocuieÈ™te toate caracterele non-alfanumerice cu liniuÈ›e
        identifier = re.sub(r'[^a-z0-9\s\.]', '-', identifier)

        # ÃnlocuieÈ™te spaÈ›iile cu liniuÈ›e
        identifier = re.sub(r'\s+', '-', identifier)

        # CurÄƒÈ›Äƒ liniuÈ›ele multiple
        identifier = re.sub(r'-+', '-', identifier)

        # EliminÄƒ liniuÈ›ele de la Ã®nceput È™i sfÃ¢rÈ™it
        identifier = identifier.strip('-')

        # LimiteazÄƒ lungimea (Archive.org are limite)
        if len(identifier) > 80:
            words = identifier.split('-')
            result_parts = []
            current_length = 0

            for word in words:
                if current_length + len(word) + 1 <= 80:  # +1 pentru liniuÈ›Äƒ
                    result_parts.append(word)
                    current_length += len(word) + 1
                else:
                    break

            identifier = '-'.join(result_parts)

        logger.debug(f"ğŸ”§ Base identifier generat: {identifier}")
        return identifier

    def remove_diacritics(self, text: str) -> str:
        """EliminÄƒ diacriticele din text"""
        import unicodedata
        return ''.join(c for c in unicodedata.normalize('NFD', text)
                      if unicodedata.category(c) != 'Mn')

    def generate_search_variants(self, title: str) -> List[str]:
        """GenereazÄƒ variante de cÄƒutare pentru un titlu"""
        variants = [title]  # Titlul original

        # Varianta fÄƒrÄƒ diacritice
        no_diacritics = self.remove_diacritics(title)
        if no_diacritics != title:
            variants.append(no_diacritics)

        # Varianta fÄƒrÄƒ punctuaÈ›ie Ã®n nume
        no_punct = re.sub(r'([A-Z])\.([A-Z])\.', r'\1 \2', title)  # "J.C." -> "J C"
        if no_punct != title:
            variants.append(no_punct)

        # Variante cu Case diferit
        variants.append(title.title())  # Title Case
        variants.append(title.lower())   # lower case

        # EliminÄƒ duplicatele pÄƒstrÃ¢nd ordinea
        seen = set()
        unique_variants = []
        for v in variants:
            if v not in seen:
                seen.add(v)
                unique_variants.append(v)

        return unique_variants[:5]  # Max 5 variante

    def test_url_exists(self, url: str, identifier: str) -> bool:
        """TesteazÄƒ dacÄƒ un URL existÄƒ pe Archive.org"""
        try:
            response = self.session.head(url, timeout=10, allow_redirects=True)
            if response.status_code == 200:
                logger.info(f"   ğŸ¯ URL EXISTENT: {identifier}")
                return True
            return False
        except:
            return False


    def scan_folders(self) -> List[Dict[str, Any]]:
        """ScaneazÄƒ folderele È™i returneazÄƒ lista de foldere de verificat"""
        folders_to_check = []
        logger.info(f"ğŸ“‚ Scanez folderul: {ARCHIVE_PATH}")

        try:
            for folder_path in ARCHIVE_PATH.iterdir():
                if not folder_path.is_dir():
                    continue

                # GÄƒseÈ™te fiÈ™ierele relevante recursiv
                relevant_files = []
                for root, dirs, files in os.walk(folder_path):
                    for file in files:
                        file_path = Path(root) / file
                        if file_path.suffix.lower() in PRIORITY_EXTENSIONS:
                            relevant_files.append(file_path)

                if not relevant_files:
                    logger.debug(f"ğŸ“ {folder_path.name}: Nu are fiÈ™iere relevante")
                    continue

                # GÄƒseÈ™te primul fiÈ™ier cu prioritatea cea mai mare
                priority_file = self.find_priority_file(relevant_files)
                if not priority_file:
                    logger.debug(f"ğŸ“ {folder_path.name}: Nu are fiÈ™iere cu extensii prioritare")
                    continue

                # CalculeazÄƒ dimensiunea totalÄƒ
                total_size = sum(f.stat().st_size for f in relevant_files if f.exists())

                # GenereazÄƒ titlurile pentru cÄƒutare
                search_title = self.clean_title_for_search(priority_file.name)
                identifier_base = self.generate_identifier_base(search_title)

                folders_to_check.append({
                    "folder_path": folder_path,
                    "priority_file": priority_file,
                    "all_files": relevant_files,
                    "total_size": total_size,
                    "search_title": search_title,
                    "identifier_base": identifier_base
                })

                logger.debug(f"ğŸ“ {folder_path.name}: titlu='{search_title}', identifier='{identifier_base}'")

            logger.info(f"ğŸ“Š GÄƒsite {len(folders_to_check)} foldere de verificat")
            return folders_to_check

        except Exception as e:
            logger.error(f"âŒ Eroare la scanarea folderelor: {e}")
            return []

    def find_priority_file(self, files: List[Path]) -> Path:
        """GÄƒseÈ™te primul fiÈ™ier conform prioritÄƒÈ›ii"""
        for ext in PRIORITY_EXTENSIONS:
            for file in files:
                if file.suffix.lower() == ext:
                    return file
        return None

    def method_1_search_duplicates(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """METODA 1 SIMPLÄ‚ - CÄƒutare directÄƒ pe titlu È™i verificare sufixe datÄƒ"""
        search_title = folder_info["search_title"]

        logger.info(f"ğŸ” METODA 1 - CÄƒutare simplÄƒ pentru: {search_title}")

        try:
            # âš¡ CÄ‚UTARE SIMPLÄ‚ fÄƒrÄƒ field restriction
            encoded_query = quote_plus(search_title)
            search_url = f"{SEARCH_BASE_URL}?query={encoded_query}"

            logger.info(f"   ğŸŒ URL cÄƒutare: {search_url}")

            response = self.session.get(search_url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()

            soup = BeautifulSoup(response.content, 'html.parser')
            title_elements = soup.find_all('h4', class_='truncated')

            if not title_elements:
                logger.info(f"   â„¹ï¸ Niciun rezultat gÄƒsit pentru '{search_title}'")
                return False, "no_results"

            logger.info(f"   ğŸ“Š GÄƒsite {len(title_elements)} rezultate")

            # âš¡ EXTRAGE IDENTIFIER-URILE din link-uri
            found_identifiers = []

            for h4 in title_elements:
                title_text = h4.get('title') or h4.get_text().strip()
                logger.info(f"      ğŸ“– Rezultat: {title_text}")

                # GÄƒseÈ™te link-ul parent sau din apropierea h4
                link_element = h4.find_parent('a') or h4.find_next('a') or h4.find_previous('a')
                if link_element and 'href' in link_element.attrs:
                    href = link_element['href']
                    # Extrage identifier-ul din href (ex: /details/certo-samuel-managementul-modern_202508)
                    if '/details/' in href:
                        identifier = href.split('/details/')[-1]
                        found_identifiers.append(identifier)
                        logger.info(f"         ğŸ”— Identifier: {identifier}")

            # âš¡ VERIFICÄ‚ SUFIXELE DE DATÄ‚
            date_suffix_identifiers = []
            for identifier in found_identifiers:
                # VerificÄƒ sufixele din COMMON_DUPLICATE_SUFFIXES
                for suffix in COMMON_DUPLICATE_SUFFIXES:
                    if identifier.endswith(suffix):
                        date_suffix_identifiers.append(identifier)
                        logger.info(f"         ğŸ¯ SUFIX DE DATÄ‚ GÄ‚SIT: {identifier}")
                        break

            if date_suffix_identifiers:
                logger.info(f"   âœ… DUPLICATE GÄ‚SITE: {len(date_suffix_identifiers)} cu sufixe de datÄƒ")
                return True, "search_duplicate_with_date_suffix"
            else:
                logger.info(f"   âœ… Rezultate gÄƒsite dar fÄƒrÄƒ sufixe de datÄƒ")
                return False, "results_no_date_suffix"

        except Exception as e:
            logger.warning(f"   âš ï¸ Eroare la cÄƒutarea HTML: {e}")
            return False, "search_error"

    def method_2_api_identifier_search(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """METODA 2: CÄƒutare prin API pentru identifier-uri cu sufixe"""
        identifier_base = folder_info["identifier_base"]

        logger.info(f"ğŸ” METODA 2 - CÄƒutare API identifier pentru: {identifier_base}*")

        params = {
            "q": f'identifier:({identifier_base}*)',
            "fl[]": "identifier",
            "rows": 100,
            "output": "json"
        }

        try:
            response = self.session.get(API_BASE_URL, params=params, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()

            data = response.json()
            response_data = data.get("response", {})
            num_found = response_data.get("numFound", 0)
            docs = response_data.get("docs", [])

            if num_found == 0:
                logger.info(f"   â„¹ï¸ Niciun identifier gÄƒsit pentru pattern-ul '{identifier_base}*'")
                return False, "no_identifiers"

            logger.info(f"   ğŸ“Š GÄƒsite {num_found} identifier-uri:")

            # âš¡ LISTÄ‚ EXTINSÄ‚ DE SUFIXE DUPLICATE
            duplicate_suffixes = [
                '-istor', '-ctrl', '-retail', '-scan', '-ocr', '-vp',
                '-retail-istor', '-scan-istor', '-ctrl-istor',
                '_202508', '_20250806', '_202507', '_202506'
            ]

            # VerificÄƒ fiecare identifier pentru sufixe duplicate
            found_duplicates = []
            for doc in docs:
                identifier = doc.get("identifier", "")
                logger.info(f"      - {identifier}")

                # VerificÄƒ dacÄƒ are sufixe de datÄƒ SAU sufixe cunoscute
                has_date_suffix = bool(re.search(r'_\d{6}$|_\d{8}$', identifier))
                has_known_suffix = any(identifier.endswith(suffix) for suffix in duplicate_suffixes)

                if has_date_suffix or has_known_suffix:
                    found_duplicates.append(identifier)
                    logger.info(f"        ğŸ¯ SUFIX DUPLICAT DETECTAT!")

            if found_duplicates:
                logger.info(f"   âœ… DUPLICATE GÄ‚SITE prin API: {len(found_duplicates)} identifier-uri cu sufixe")
                for dup in found_duplicates[:3]:
                    logger.info(f"      ğŸ”¸ {dup}")
                return True, "api_duplicate"
            else:
                logger.info(f"   âœ… Identifier-uri gÄƒsite dar fÄƒrÄƒ sufixe duplicate")
                return False, "no_suffix_duplicates"

        except Exception as e:
            logger.error(f"   âŒ Eroare la API search: {e}")
            return False, "api_error"

    def method_3_direct_url_test(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """METODA 3 GENERALÄ‚ - Test direct URL-uri cu variante inteligente"""
        identifier_base = folder_info["identifier_base"]

        logger.info(f"ğŸ” METODA 3 - Test rapid URL pentru: {identifier_base}")

        found_urls = []

        # âš¡ GENEREAZÄ‚ VARIANTE INTELIGENTE
        quick_variants = [
            identifier_base,  # Varianta de bazÄƒ
        ]



        # âš¡ ADAUGÄ‚ variante cu conÈ›inutul din paranteze (dacÄƒ existÄƒ)
        if hasattr(self, '_parentheses_content') and self._parentheses_content:
            parentheses_clean = re.sub(r'[^\w\s]', '', self._parentheses_content.lower())  # "trad." -> "trad"
            parentheses_with_dot = self._parentheses_content.lower()  # "trad."

            logger.debug(f"ğŸ” Generez variante cu paranteze: '{parentheses_clean}' È™i '{parentheses_with_dot}'")

            # GÄƒseÈ™te primul " - " È™i insereazÄƒ conÈ›inutul din paranteze
            if ' - ' in identifier_base:
                parts = identifier_base.split(' - ', 1)
                base_part = parts[0]  # "chelaru-marius"
                rest_part = parts[1]  # "poarta-catre-poezia-araba"

                # AdaugÄƒ variante cu conÈ›inutul din paranteze
                quick_variants.extend([
                    f"{base_part}-{parentheses_clean}-{rest_part}",      # chelaru-marius-trad-poarta...
                    f"{base_part}-{parentheses_with_dot}-{rest_part}",   # chelaru-marius-trad.-poarta...
                ])

                logger.debug(f"ğŸ”§ Variante cu paranteze generate:")
                logger.debug(f"   - {base_part}-{parentheses_clean}-{rest_part}")
                logger.debug(f"   - {base_part}-{parentheses_with_dot}-{rest_part}")

        # âš¡ ADAUGÄ‚ sufixe comune pentru toate variantele
        base_variants = quick_variants.copy()
        for base in base_variants:
            quick_variants.extend([
                base + '-ctrl',
                base + '-retail',
                base + '-scan',
                base + '-ocr',
                base + '-vp',
            ])

        # âš¡ ADAUGÄ‚ variante cu modificÄƒri de punctuaÈ›ie
        extra_variants = []
        for variant in quick_variants:
            extra_variants.extend([
                variant.replace('j.c.', 'j.-c.'),
                variant.replace('-', ''),
                variant.replace('.', ''),
            ])
        quick_variants.extend(extra_variants)

        # âš¡ ELIMINÄ‚ DUPLICATELE
        quick_variants = list(dict.fromkeys(quick_variants))  # PÄƒstreazÄƒ ordinea

        logger.debug(f"ğŸ” Testez {len(quick_variants)} variante pentru {identifier_base}")

        # âš¡ TESTEAZÄ‚ TOATE VARIANTELE
        for variant in quick_variants:
            test_url = f"{DETAILS_BASE_URL}/{variant}"
            if self.test_url_exists(test_url, variant):
                found_urls.append(variant)
                logger.info(f"   ğŸ¯ GÄ‚SIT: {variant}")
                break  # Stop la primul gÄƒsit

        if found_urls:
            logger.info(f"   âœ… DUPLICAT GÄ‚SIT: {found_urls[0]}")
            return True, "url_duplicate"
        else:
            logger.info(f"   âœ… Niciun duplicat gÄƒsit")
            return False, "no_duplicates"

    def method_3_direct_url_test_enhanced(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """METODA 3 ÃMBUNÄ‚TÄ‚ÈšITÄ‚ - Test direct URL cu toate variantele posibile"""
        identifier_base = folder_info["identifier_base"]
        search_title = folder_info["search_title"]

        logger.info(f"ğŸ” METODA 3 ÃMBUNÄ‚TÄ‚ÈšITÄ‚ - Test URL pentru: {identifier_base}")

        # âš¡ GENEREAZÄ‚ TOATE VARIANTELE POSIBILE
        variants_to_test = [
            identifier_base,  # Base original
        ]

        # âš¡ ADAUGÄ‚ VARIANTE cu sufixe comune
        common_suffixes = ['-retail', '-scan', '-ctrl', '-ocr', '-vp', '-istor']
        for suffix in common_suffixes:
            variants_to_test.append(identifier_base + suffix)

        # âš¡ ADAUGÄ‚ VARIANTE cu cuvinte comune care lipsesc
        # Pentru cazuri ca "trad", "ed", "edition" etc.
        common_middle_words = ['trad', 'trad.', 'ed', 'edition', 'vol', 'tome']

        # ÃncearcÄƒ sÄƒ insereze cuvinte Ã®n mijloc (dupÄƒ numele autorului)
        parts = identifier_base.split('-')
        if len(parts) >= 3:  # autor-nume-titlu...
            base_author = '-'.join(parts[:2])  # "chelaru-marius"
            rest_title = '-'.join(parts[2:])   # "poarta-catre-poezia-araba"

            for word in common_middle_words:
                # Varianta cu cuvÃ¢ntul Ã®n mijloc
                middle_variant = f"{base_author}-{word}-{rest_title}"
                variants_to_test.append(middle_variant)

                # È˜i cu sufixe
                for suffix in common_suffixes:
                    variants_to_test.append(middle_variant + suffix)

        # âš¡ ADAUGÄ‚ VARIANTE cu COMMON_DUPLICATE_SUFFIXES
        for date_suffix in COMMON_DUPLICATE_SUFFIXES[:6]:  # Primele 6 sufixe
            variants_to_test.append(identifier_base + date_suffix)

        # âš¡ ELIMINÄ‚ DUPLICATELE
        variants_to_test = list(dict.fromkeys(variants_to_test))

        logger.info(f"   ğŸ” Testez {len(variants_to_test)} variante")

        # âš¡ TESTEAZÄ‚ TOATE VARIANTELE
        for i, variant in enumerate(variants_to_test):
            if i % 10 == 0:  # Log la fiecare 10 variante
                logger.info(f"   ğŸ“Š Progres teste URL: {i+1}/{len(variants_to_test)}")

            test_url = f"{DETAILS_BASE_URL}/{variant}"
            if self.test_url_exists(test_url, variant):
                logger.info(f"   ğŸ¯ DUPLICAT GÄ‚SIT: {variant}")
                return True, f"url_variant_{i}"

        logger.info(f"   âœ… Niciun duplicat gÄƒsit Ã®n {len(variants_to_test)} variante")
        return False, "no_duplicates"

    def check_folder_web_only(self, folder_info: Dict[str, Any]) -> bool:
        """VerificÄƒ un folder folosind doar metodele web (fÄƒrÄƒ Chrome)"""
        folder_path = folder_info["folder_path"]

        logger.info(f"\nğŸ“‚ Verific folderul: {folder_path.name}")
        logger.info(f"ğŸ“„ Titlu cÄƒutare: {folder_info['search_title']}")
        logger.info(f"ğŸ”— Base identifier: {folder_info['identifier_base']}")

        # Sari peste folderele deja procesate
        folder_str = str(folder_path)
        if folder_str in self.state["processed_folders"]:
            logger.info(f"â­ï¸ Folder deja procesat, sar peste")
            return False

        try:
            # âš¡ NU MAI FOLOSI SEARCH HTML - e blocat de Archive.org

            # PRIMUL: Test direct URL cu variante inteligente
            is_duplicate, method = self.method_3_direct_url_test_enhanced(folder_info)
            if is_duplicate:
                logger.info(f"ğŸ¯ DUPLICAT DETECTAT prin test direct URL!")
                self.url_test_hits += 1
                self.delete_folder(folder_info, f"Duplicat gÄƒsit prin {method}")
                return True

            # AL DOILEA: API search
            is_duplicate, method = self.method_2_api_identifier_search(folder_info)
            if is_duplicate and method == "api_duplicate":
                logger.info("ğŸ¯ DUPLICAT DETECTAT prin API identifier!")
                self.api_hits += 1
                self.delete_folder(folder_info, "Duplicat gÄƒsit prin API identifier")
                return True

            # Niciun duplicat gÄƒsit
            logger.info("âœ… NU sunt duplicate detectate - pÄƒstrez folderul")
            self.state["processed_folders"].append(folder_str)
            self.checked_count += 1
            self.state["stats"]["total_checked"] += 1
            return False

        except Exception as e:
            logger.error(f"âŒ Eroare la verificarea folderului {folder_path.name}: {e}")
            self.error_count += 1
            return False

    def delete_folder(self, folder_info: Dict[str, Any], reason: str):
        """È˜terge folderul cu duplicat"""
        folder_path = folder_info["folder_path"]

        try:
            logger.warning(f"ğŸ—‘ï¸ È˜terg folderul duplicat: {folder_path}")
            logger.info(f"   Motiv: {reason}")

            # CalculeazÄƒ spaÈ›iul eliberat
            size_mb = folder_info.get('total_size', 0) / (1024 * 1024)

            # CreeazÄƒ backup info
            backup_info = {
                "folder": str(folder_path),
                "priority_file": str(folder_info["priority_file"]),
                "search_title": folder_info["search_title"],
                "identifier_base": folder_info["identifier_base"],
                "all_files": [str(f) for f in folder_info["all_files"]],
                "size_mb": round(size_mb, 2),
                "deleted_at": datetime.now().isoformat(),
                "reason": reason
            }

            # È˜terge folderul
            shutil.rmtree(folder_path)

            # ActualizeazÄƒ statisticile
            if "deleted_folders" not in self.state:
                self.state["deleted_folders"] = []

            self.state["deleted_folders"].append(backup_info)
            self.deleted_count += 1

            if "stats" not in self.state:
                self.state["stats"] = {
                    "total_checked": 0, "total_deleted": 0, "total_space_saved_mb": 0.0,
                    "search_hits": 0, "api_hits": 0, "url_test_hits": 0
                }

            self.state["stats"]["total_deleted"] += 1
            self.state["stats"]["total_space_saved_mb"] = round(
                self.state["stats"]["total_space_saved_mb"] + size_mb, 2
            )

            logger.info(f"âœ… Folder È™ters cu succes! ({size_mb:.2f} MB eliberat)")

            # SalveazÄƒ starea imediat
            self.save_state()

        except Exception as e:
            logger.error(f"âŒ Eroare la È™tergerea folderului: {e}")
            self.error_count += 1

    def generate_report(self):
        """GenereazÄƒ raportul final"""
        total_processed = self.checked_count + self.deleted_count

        report_lines = [
            "\n" + "=" * 60,
            "ğŸ“Š RAPORT FINAL - Pure Web Archive Checker",
            "=" * 60,
            f"âœ… Foldere verificate: {self.checked_count}",
            f"ğŸ—‘ï¸ Foldere È™terse (duplicate): {self.deleted_count}",
            f"âŒ Erori Ã®ntÃ¢mpinate: {self.error_count}",
            f"ğŸ’¾ SpaÈ›iu eliberat Ã®n aceastÄƒ sesiune: {sum(f.get('size_mb', 0) for f in self.state['deleted_folders'][-self.deleted_count:]):.2f} MB",
            "",
            "ğŸ“ˆ STATISTICI METODE DETECTARE:",
            f"ğŸ” Duplicate gÄƒsite prin cÄƒutare web: {self.search_hits}",
            f"ğŸ”§ Duplicate gÄƒsite prin API identifier: {self.api_hits}",
            f"ğŸŒ Duplicate gÄƒsite prin test direct URL: {self.url_test_hits}",
            "",
            "ğŸ“Š EFICIENÈšÄ‚ METODE:",
            f"ğŸ” CÄƒutare web: {(self.search_hits / max(total_processed, 1)) * 100:.1f}%",
            f"ğŸ”§ API identifier: {(self.api_hits / max(total_processed, 1)) * 100:.1f}%",
            f"ğŸŒ Test direct URL: {(self.url_test_hits / max(total_processed, 1)) * 100:.1f}%",
            "",
            f"ğŸ“ˆ STATISTICI TOTALE:",
            f"ğŸ“ˆ Total istoric foldere È™terse: {self.state['stats']['total_deleted']}",
            f"ğŸ“ˆ Total spaÈ›iu istoric eliberat: {self.state['stats']['total_space_saved_mb']:.2f} MB",
            "=" * 60
        ]

        for line in report_lines:
            logger.info(line)

        if self.deleted_count > 0:
            logger.info(f"\nğŸ“‹ FOLDERE È˜TERSE ÃN ACEASTÄ‚ SESIUNE:")
            recent_deleted = self.state["deleted_folders"][-self.deleted_count:]
            for i, folder_info in enumerate(recent_deleted, 1):
                logger.info(f"   {i}. {Path(folder_info['folder']).name}")
                logger.info(f"      Titlu: {folder_info.get('search_title', 'N/A')}")
                logger.info(f"      Motiv: {folder_info.get('reason', 'N/A')}")
                logger.info(f"      SpaÈ›iu eliberat: {folder_info.get('size_mb', 0):.2f} MB")

    def test_specific_chatterji(self):
        """Test specific pentru cartea Chatterji"""
        print("ğŸ” TESTEZ DIRECT URL-ul cunoscut:")

        known_identifier = "chatterji-j.-c.-filozofia-ezoterica-a-indiei-ctrl"
        test_url = f"https://archive.org/details/{known_identifier}"

        try:
            response = self.session.head(test_url, timeout=15)
            if response.status_code == 200:
                print(f"   âœ… URL-ul existÄƒ: {known_identifier}")
                print(f"   ğŸŒ {test_url}")
            else:
                print(f"   âŒ Status {response.status_code} pentru {known_identifier}")
        except Exception as e:
            print(f"   âŒ Eroare la testarea URL: {e}")

        print("\nğŸ” TESTEZ API DIRECT (funcÈ›ioneazÄƒ):")
        try:
            api_url = "https://archive.org/advancedsearch.php"
            params = {
                "q": "chatterji",
                "fl[]": ["identifier", "title"],
                "rows": 5,
                "output": "json"
            }

            response = self.session.get(api_url, params=params, timeout=15)
            if response.status_code == 200:
                data = response.json()
                docs = data.get("response", {}).get("docs", [])
                print(f"   âœ… API gÄƒsit {len(docs)} rezultate pentru 'chatterji'")

                for doc in docs[:3]:
                    print(f"      ğŸ“š {doc.get('identifier', 'N/A')}: {doc.get('title', 'N/A')}")
            else:
                print(f"   âŒ API Status: {response.status_code}")
        except Exception as e:
            print(f"   âŒ Eroare API: {e}")

        print(f"\nâš ï¸ NOTÄ‚: Search-ul HTML prin requests este blocat de Archive.org")
        print(f"   Dar API-ul È™i testul direct URL funcÈ›ioneazÄƒ perfect!")

    def run(self):
        """RuleazÄƒ procesul principal"""
        logger.info("=" * 60)
        logger.info("ğŸš€ START - Archive.org Pure Web Duplicate Checker")
        logger.info(f"ğŸ“ Folder verificat: {ARCHIVE_PATH}")
        logger.info("âš¡ MetodÄƒ: 100% Web-based (fÄƒrÄƒ Chrome)")
        logger.info("ğŸ” 3 strategii: Search web + API identifier + Test direct URL")
        logger.info("=" * 60)

        # VerificÄƒ folderul sursÄƒ
        if not ARCHIVE_PATH.exists():
            logger.error(f"âŒ Folderul {ARCHIVE_PATH} nu existÄƒ!")
            return False

        try:
            # ScaneazÄƒ folderele
            folders_to_check = self.scan_folders()

            if not folders_to_check:
                logger.info("âœ… Nu sunt foldere de verificat!")
                return True

            # ProceseazÄƒ fiecare folder
            for i, folder_info in enumerate(folders_to_check, 1):
                logger.info(f"\nğŸ“Š Progres: {i}/{len(folders_to_check)}")

                try:
                    self.check_folder_web_only(folder_info)

                    # PauzÄƒ Ã®ntre verificÄƒri pentru a nu suprasolicita serverul
                    if i < len(folders_to_check):
                        logger.info(f"â³ PauzÄƒ {DELAY_BETWEEN_REQUESTS} secunde...")
                        time.sleep(DELAY_BETWEEN_REQUESTS)

                except KeyboardInterrupt:
                    logger.warning("\nâš ï¸ Ãntrerupt de utilizator")
                    break
                except Exception as e:
                    logger.error(f"âŒ Eroare la procesarea folderului: {e}")
                    continue

            # SalveazÄƒ starea finalÄƒ
            self.save_state()

            # GenereazÄƒ raportul
            self.generate_report()

            return True

        except KeyboardInterrupt:
            logger.warning("\nâš ï¸ Proces Ã®ntrerupt manual")
            self.save_state()
            self.generate_report()
            return False
        except Exception as e:
            logger.error(f"\nâŒ Eroare fatalÄƒ: {e}", exc_info=True)
            self.save_state()
            return False


def main():
    """FuncÈ›ia principalÄƒ"""
    try:
        checker = ArchivePureWebChecker()

        # Test specific pentru Chatterji
        print("ğŸ§ª TESTEZ CAZUL CHATTERJI:")
        checker.test_specific_chatterji()
        print("\n" + "="*60 + "\n")

        # âš ï¸ ADAUGÄ‚ ASTA pentru a reprocessa folderul Chatterji
        if "g:\\ARHIVA\\C\\Chatterji, J.C" in checker.state["processed_folders"]:
            print("ğŸ”„ Elimin Chatterji din folderele procesate pentru retestare...")
            checker.state["processed_folders"].remove("g:\\ARHIVA\\C\\Chatterji, J.C")
            checker.save_state()

        success = checker.run()

        if success:
            print("\nÃ¢Å“â€¦ Proces finalizat cu succes!")
        else:
            print("\nÃ¢Å’ Proces finalizat cu erori!")

    except Exception as e:
        print(f"Ã¢Å’ Eroare fatalÃ„Æ’: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()