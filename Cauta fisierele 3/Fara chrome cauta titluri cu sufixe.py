#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Internet Archive Duplicate Checker - Pure Web Method (No Chrome Required)
DetecteazÄƒ duplicate pe Archive.org folosind doar HTTP requests È™i parsing HTML.

Strategiile de detectare:
1. CÄƒutare multiplÄƒ pe archive.org/search cu variante de titluri
2. CÄƒutare directÄƒ pe pattern-uri de identifier-uri cu wildcard
3. Test direct de existenÈ›Äƒ URL pentru sufixe comune

AVANTAJE:
- Nu necesitÄƒ Chrome sau Selenium
- Mult mai rapid decÃ¢t simularea upload
- FuncÈ›ioneazÄƒ pe orice sistem
- Detectare inteligentÄƒ a duplicatelor
"""

import os
import re
import time
import shutil
import json
import logging
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Tuple, Optional
from urllib.parse import quote_plus
from bs4 import BeautifulSoup

# ============= CONFIGURÄ‚RI =============
ARCHIVE_PATH = Path(r"g:\ARHIVA\C")
SEARCH_BASE_URL = "https://archive.org/search"
DETAILS_BASE_URL = "https://archive.org/details"
API_BASE_URL = "https://archive.org/advancedsearch.php"

STATE_FILE = Path("archive_duplicate_pure_web_state.json")
LOG_FILE = Path(f"archive_duplicate_web_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")

# Extensii prioritare pentru verificare (Ã®n ordinea prioritÄƒÈ›ii)
PRIORITY_EXTENSIONS = ['.pdf', '.epub', '.mobi', '.djvu', '.docx', '.doc', '.lit', '.rtf']

# Extensii de ignorat
IGNORE_EXTENSIONS = ['.jpg', '.png']

# Configurare requests
MAX_RETRIES = 3
REQUEST_TIMEOUT = 15
DELAY_BETWEEN_REQUESTS = 1.5  # Secunde Ã®ntre requests pentru a nu suprasolicita serverul

# Sufixe comune de duplicate de testat
COMMON_DUPLICATE_SUFFIXES = [
    '_202508', '_20250806', '_202507', '_20250705', '_202506', '_20250604',
    '_202505', '_20250503', '_202504', '_20250402', '_202503', '_20250301',
    '_202502', '_20250201', '_202501', '_20250101', '_202412', '_20241201'
]

# ============= CONFIGURARE LOGGER =============
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ArchivePureWebChecker")


class ArchivePureWebChecker:
    def __init__(self):
        self.state = self.load_state()
        self.deleted_count = 0
        self.checked_count = 0
        self.error_count = 0

        # Statistici metode
        self.search_hits = 0
        self.api_hits = 0
        self.url_test_hits = 0

        # Session pentru requests cu retry È™i headers
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })

        # Adapter cu retry strategy
        from requests.adapters import HTTPAdapter
        from urllib3.util.retry import Retry

        retry_strategy = Retry(
            total=MAX_RETRIES,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)

    def load_state(self) -> Dict[str, Any]:
        """ÃncarcÄƒ starea salvatÄƒ anterior"""
        default_state = {
            "processed_folders": [],
            "deleted_folders": [],
            "last_processed": None,
            "stats": {
                "total_checked": 0,
                "total_deleted": 0,
                "total_space_saved_mb": 0.0,
                "search_hits": 0,
                "api_hits": 0,
                "url_test_hits": 0
            }
        }

        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    state = json.load(f)

                    # Compatibilitate cu versiuni anterioare
                    for key in default_state:
                        if key not in state:
                            state[key] = default_state[key]

                    for key in default_state["stats"]:
                        if key not in state["stats"]:
                            state["stats"][key] = default_state["stats"][key]

                    logger.info(f"ğŸ“‹ Stare Ã®ncÄƒrcatÄƒ: {len(state.get('processed_folders', []))} foldere procesate anterior")
                    return state
            except Exception as e:
                logger.warning(f"âš ï¸ Nu s-a putut Ã®ncÄƒrca starea: {e}")

        return default_state

    def save_state(self):
        """SalveazÄƒ starea curentÄƒ"""
        self.state["last_processed"] = datetime.now().isoformat()
        self.state["stats"]["search_hits"] = self.search_hits
        self.state["stats"]["api_hits"] = self.api_hits
        self.state["stats"]["url_test_hits"] = self.url_test_hits

        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)
            logger.debug("ğŸ’¾ Stare salvatÄƒ cu succes")
        except Exception as e:
            logger.error(f"âŒ Eroare la salvarea stÄƒrii: {e}")

    def clean_title_for_search(self, filename: str) -> str:
        """CurÄƒÈ›Äƒ titlul pentru cÄƒutare pe Archive.org"""
        name = Path(filename).stem
        logger.debug(f"ğŸ”§ CurÄƒÈ›are titlu original: {filename}")

        # EliminÄƒ sufixele de data
        name = re.sub(r'[_-]\d{6,8}$', '', name)

        # EliminÄƒ parantezele È™i conÈ›inutul lor
        name = re.sub(r'\s*\([^)]*\)', '', name)
        name = re.sub(r'\s*\[[^\]]*\]', '', name)

        # EliminÄƒ versiunile
        name = re.sub(r'\s*[-â€“]\s*[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?(?:\s*[-â€“]\s*\d+)?$', '', name)
        name = re.sub(r'\s+[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?$', '', name)

        # EliminÄƒ sufixele tehnice
        suffixes = ['scan', 'ctrl', 'retail', r'cop\d+', 'Vp', 'draft', 'final',
                   'ocr', 'OCR', 'edit', 'edited', 'rev', 'revised', 'proof',
                   'beta', 'alpha', 'test', 'demo', 'sample', 'preview',
                   'full', 'complete', 'fix', 'fixed', 'corrected']

        # EliminÄƒ sufixele care apar dupÄƒ ultima " - "
        parts = name.rsplit(' - ', 1)
        if len(parts) == 2:
            last_part = parts[1].strip()
            suffix_pattern = '|'.join(suffixes)
            if re.match(f'^({suffix_pattern})$', last_part, re.IGNORECASE):
                name = parts[0]

        # EliminÄƒ #TAGS (ex: #ISTOR)
        name = re.sub(r'\s*#\w+', '', name)

        # CurÄƒÈ›Äƒ spaÈ›iile
        name = re.sub(r'\s+', ' ', name).strip()
        name = re.sub(r'\s*[-â€“]\s*$', '', name)

        logger.debug(f"âœ¨ Titlu curÄƒÈ›at: {name}")
        return name

    def generate_identifier_base(self, title: str) -> str:
        """GenereazÄƒ base identifier-ul aÈ™a cum l-ar genera Archive.org"""
        # Ãncepe cu titlul
        identifier = title.lower()

        # ÃnlocuieÈ™te toate caracterele non-alfanumerice cu liniuÈ›e
        identifier = re.sub(r'[^a-z0-9\s\.]', '-', identifier)

        # ÃnlocuieÈ™te spaÈ›iile cu liniuÈ›e
        identifier = re.sub(r'\s+', '-', identifier)

        # CurÄƒÈ›Äƒ liniuÈ›ele multiple
        identifier = re.sub(r'-+', '-', identifier)

        # EliminÄƒ liniuÈ›ele de la Ã®nceput È™i sfÃ¢rÈ™it
        identifier = identifier.strip('-')

        # LimiteazÄƒ lungimea (Archive.org are limite)
        if len(identifier) > 80:
            words = identifier.split('-')
            result_parts = []
            current_length = 0

            for word in words:
                if current_length + len(word) + 1 <= 80:  # +1 pentru liniuÈ›Äƒ
                    result_parts.append(word)
                    current_length += len(word) + 1
                else:
                    break

            identifier = '-'.join(result_parts)

        logger.debug(f"ğŸ”§ Base identifier generat: {identifier}")
        return identifier

    def scan_folders(self) -> List[Dict[str, Any]]:
        """ScaneazÄƒ folderele È™i returneazÄƒ lista de foldere de verificat"""
        folders_to_check = []
        logger.info(f"ğŸ“‚ Scanez folderul: {ARCHIVE_PATH}")

        try:
            for folder_path in ARCHIVE_PATH.iterdir():
                if not folder_path.is_dir():
                    continue

                # GÄƒseÈ™te fiÈ™ierele relevante recursiv
                relevant_files = []
                for root, dirs, files in os.walk(folder_path):
                    for file in files:
                        file_path = Path(root) / file
                        if file_path.suffix.lower() in PRIORITY_EXTENSIONS:
                            relevant_files.append(file_path)

                if not relevant_files:
                    logger.debug(f"ğŸ“ {folder_path.name}: Nu are fiÈ™iere relevante")
                    continue

                # GÄƒseÈ™te primul fiÈ™ier cu prioritatea cea mai mare
                priority_file = self.find_priority_file(relevant_files)
                if not priority_file:
                    logger.debug(f"ğŸ“ {folder_path.name}: Nu are fiÈ™iere cu extensii prioritare")
                    continue

                # CalculeazÄƒ dimensiunea totalÄƒ
                total_size = sum(f.stat().st_size for f in relevant_files if f.exists())

                # GenereazÄƒ titlurile pentru cÄƒutare
                search_title = self.clean_title_for_search(priority_file.name)
                identifier_base = self.generate_identifier_base(search_title)

                folders_to_check.append({
                    "folder_path": folder_path,
                    "priority_file": priority_file,
                    "all_files": relevant_files,
                    "total_size": total_size,
                    "search_title": search_title,
                    "identifier_base": identifier_base
                })

                logger.debug(f"ğŸ“ {folder_path.name}: titlu='{search_title}', identifier='{identifier_base}'")

            logger.info(f"ğŸ“Š GÄƒsite {len(folders_to_check)} foldere de verificat")
            return folders_to_check

        except Exception as e:
            logger.error(f"âŒ Eroare la scanarea folderelor: {e}")
            return []

    def find_priority_file(self, files: List[Path]) -> Path:
        """GÄƒseÈ™te primul fiÈ™ier conform prioritÄƒÈ›ii"""
        for ext in PRIORITY_EXTENSIONS:
            for file in files:
                if file.suffix.lower() == ext:
                    return file
        return None

    def method_1_search_duplicates(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """METODA 1: CÄƒutare pe archive.org/search pentru duplicate"""
        search_title = folder_info["search_title"]

        logger.info(f"ğŸ” METODA 1 - CÄƒutare duplicate pentru: {search_title}")

        # ConstruieÈ™te URL-ul de cÄƒutare
        query = quote_plus(search_title)
        search_url = f"{SEARCH_BASE_URL}?query={query}"

        logger.debug(f"ğŸŒ URL cÄƒutare: {search_url}")

        try:
            response = self.session.get(search_url, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()

            # Parse HTML cu BeautifulSoup
            soup = BeautifulSoup(response.content, 'html.parser')

            # GÄƒseÈ™te toate elementele <h4> cu clasa "truncated"
            title_elements = soup.find_all('h4', class_='truncated')

            if not title_elements:
                logger.info(f"   â„¹ï¸ Niciun rezultat gÄƒsit Ã®n cÄƒutare")
                return False, "no_results"

            # Extrage titlurile È™i verificÄƒ duplicate
            found_titles = []
            for h4 in title_elements:
                title_attr = h4.get('title', '').strip()
                title_text = h4.get_text().strip()

                # FoloseÈ™te titlul din attribute sau text
                final_title = title_attr if title_attr else title_text
                if final_title:
                    found_titles.append(final_title)

            logger.info(f"   ğŸ“Š GÄƒsite {len(found_titles)} rezultate:")
            for i, title in enumerate(found_titles[:5], 1):  # AfiÈ™eazÄƒ primele 5
                logger.info(f"      {i}. {title}")

            # VerificÄƒ duplicate - cÄƒutÄƒ titluri identice
            if len(found_titles) >= 2:
                # VerificÄƒ dacÄƒ sunt cel puÈ›in 2 titluri identice
                title_counts = {}
                for title in found_titles:
                    # NormalizeazÄƒ titlul pentru comparaÈ›ie
                    normalized = re.sub(r'[^\w\s]', ' ', title.lower())
                    normalized = re.sub(r'\s+', ' ', normalized).strip()

                    title_counts[normalized] = title_counts.get(normalized, 0) + 1

                # VerificÄƒ dacÄƒ vreun titlu apare de mai multe ori
                for title, count in title_counts.items():
                    if count >= 2:
                        logger.info(f"   ğŸ¯ DUPLICAT GÄ‚SIT prin cÄƒutare!")
                        logger.info(f"      Titlu duplicat: {title} (apare de {count} ori)")
                        return True, "search_duplicate"

            logger.info(f"   âœ… Un singur rezultat gÄƒsit - nu sunt duplicate evidente")
            return False, "single_result"

        except requests.RequestException as e:
            logger.warning(f"   âš ï¸ Eroare la cÄƒutarea web: {e}")
            return False, "search_error"
        except Exception as e:
            logger.error(f"   âŒ Eroare neaÈ™teptatÄƒ la cÄƒutarea web: {e}")
            return False, "search_error"

    def method_2_api_identifier_search(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """METODA 2: CÄƒutare prin API pentru identifier-uri cu sufixe"""
        identifier_base = folder_info["identifier_base"]

        logger.info(f"ğŸ” METODA 2 - CÄƒutare API identifier pentru: {identifier_base}*")

        # CÄƒutare prin API pentru identifier-uri care Ã®ncep cu base-ul nostru
        params = {
            "q": f'identifier:({identifier_base}*)',
            "fl[]": "identifier",
            "rows": 100,
            "output": "json"
        }

        try:
            response = self.session.get(API_BASE_URL, params=params, timeout=REQUEST_TIMEOUT)
            response.raise_for_status()

            data = response.json()
            response_data = data.get("response", {})
            num_found = response_data.get("numFound", 0)
            docs = response_data.get("docs", [])

            if num_found == 0:
                logger.info(f"   â„¹ï¸ Niciun identifier gÄƒsit pentru pattern-ul '{identifier_base}*'")
                return False, "no_identifiers"

            logger.info(f"   ğŸ“Š GÄƒsite {num_found} identifier-uri:")

            # VerificÄƒ fiecare identifier pentru sufixe duplicate
            found_duplicates = []
            for doc in docs:
                identifier = doc.get("identifier", "")
                logger.info(f"      - {identifier}")

                # VerificÄƒ dacÄƒ are sufixe de datÄƒ
                if re.search(r'_\d{6}$|_\d{8}$', identifier):
                    found_duplicates.append(identifier)
                    logger.info(f"        ğŸ¯ SUFIX DUPLICAT DETECTAT!")

            if found_duplicates:
                logger.info(f"   âœ… DUPLICATE GÄ‚SITE prin API: {len(found_duplicates)} identifier-uri cu sufixe")
                for dup in found_duplicates[:3]:  # AfiÈ™eazÄƒ primele 3
                    logger.info(f"      ğŸ”¸ {dup}")
                return True, "api_duplicate"
            else:
                logger.info(f"   âœ… Identifier-uri gÄƒsite dar fÄƒrÄƒ sufixe duplicate")
                return False, "no_suffix_duplicates"

        except requests.RequestException as e:
            logger.warning(f"   âš ï¸ Eroare la API search: {e}")
            return False, "api_error"
        except Exception as e:
            logger.error(f"   âŒ Eroare neaÈ™teptatÄƒ la API search: {e}")
            return False, "api_error"

    def method_3_direct_url_test(self, folder_info: Dict[str, Any]) -> Tuple[bool, str]:
        """METODA 3: Test direct de existenÈ›Äƒ URL pentru sufixe comune"""
        identifier_base = folder_info["identifier_base"]

        logger.info(f"ğŸ” METODA 3 - Test direct URL pentru: {identifier_base}")

        found_urls = []

        # TesteazÄƒ sufixele comune de duplicate
        for suffix in COMMON_DUPLICATE_SUFFIXES[:8]:  # TesteazÄƒ primele 8 sufixe
            test_identifier = f"{identifier_base}{suffix}"
            test_url = f"{DETAILS_BASE_URL}/{test_identifier}"

            logger.debug(f"   ğŸ”— Testez: {test_url}")

            try:
                # HEAD request pentru a nu descÄƒrca conÈ›inutul
                response = self.session.head(test_url, timeout=REQUEST_TIMEOUT, allow_redirects=True)

                if response.status_code == 200:
                    logger.info(f"   ğŸ¯ URL EXISTENT: {test_identifier}")
                    found_urls.append(test_identifier)
                elif response.status_code == 404:
                    logger.debug(f"   âŒ Nu existÄƒ: {test_identifier}")
                else:
                    logger.debug(f"   âš ï¸ Status {response.status_code}: {test_identifier}")

            except requests.RequestException as e:
                logger.debug(f"   âŒ Eroare pentru {test_identifier}: {e}")

            # MicÄƒ pauzÄƒ Ã®ntre teste
            time.sleep(0.3)

        if found_urls:
            logger.info(f"   âœ… DUPLICATE GÄ‚SITE prin test URL: {len(found_urls)} URL-uri existente")
            for url in found_urls[:3]:  # AfiÈ™eazÄƒ primele 3
                logger.info(f"      ğŸ”¸ {url}")
            return True, "url_duplicate"
        else:
            logger.info(f"   âœ… Niciun URL duplicat gÄƒsit prin test direct")
            return False, "no_url_duplicates"

    def check_folder_web_only(self, folder_info: Dict[str, Any]) -> bool:
        """VerificÄƒ un folder folosind doar metodele web (fÄƒrÄƒ Chrome)"""
        folder_path = folder_info["folder_path"]

        logger.info(f"\nğŸ“‚ Verific folderul: {folder_path.name}")
        logger.info(f"ğŸ“„ Titlu cÄƒutare: {folder_info['search_title']}")
        logger.info(f"ğŸ”— Base identifier: {folder_info['identifier_base']}")

        # Sari peste folderele deja procesate
        folder_str = str(folder_path)
        if folder_str in self.state["processed_folders"]:
            logger.info(f"â­ï¸ Folder deja procesat, sar peste")
            return False

        try:
            # METODA 1: CÄƒutare pe archive.org/search
            is_duplicate, method = self.method_1_search_duplicates(folder_info)

            if is_duplicate and method == "search_duplicate":
                logger.info("ğŸ¯ DUPLICAT DETECTAT prin cÄƒutare web!")
                self.search_hits += 1
                self.delete_folder(folder_info, "Duplicat gÄƒsit prin cÄƒutare web")
                return True

            time.sleep(DELAY_BETWEEN_REQUESTS)  # PauzÄƒ Ã®ntre metode

            # METODA 2: CÄƒutare prin API pentru identifier-uri
            is_duplicate, method = self.method_2_api_identifier_search(folder_info)

            if is_duplicate and method == "api_duplicate":
                logger.info("ğŸ¯ DUPLICAT DETECTAT prin API identifier!")
                self.api_hits += 1
                self.delete_folder(folder_info, "Duplicat gÄƒsit prin API identifier")
                return True

            time.sleep(DELAY_BETWEEN_REQUESTS)  # PauzÄƒ Ã®ntre metode

            # METODA 3: Test direct URL
            is_duplicate, method = self.method_3_direct_url_test(folder_info)

            if is_duplicate and method == "url_duplicate":
                logger.info("ğŸ¯ DUPLICAT DETECTAT prin test direct URL!")
                self.url_test_hits += 1
                self.delete_folder(folder_info, "Duplicat gÄƒsit prin test direct URL")
                return True

            # DacÄƒ nicio metodÄƒ nu a gÄƒsit duplicate
            logger.info("âœ… NU sunt duplicate detectate - pÄƒstrez folderul")

            # MarcheazÄƒ ca procesat
            self.state["processed_folders"].append(folder_str)
            self.checked_count += 1
            self.state["stats"]["total_checked"] += 1

            return False

        except Exception as e:
            logger.error(f"âŒ Eroare la verificarea folderului {folder_path.name}: {e}")
            self.error_count += 1
            return False

    def delete_folder(self, folder_info: Dict[str, Any], reason: str):
        """È˜terge folderul cu duplicat"""
        folder_path = folder_info["folder_path"]

        try:
            logger.warning(f"ğŸ—‘ï¸ È˜terg folderul duplicat: {folder_path}")
            logger.info(f"   Motiv: {reason}")

            # CalculeazÄƒ spaÈ›iul eliberat
            size_mb = folder_info.get('total_size', 0) / (1024 * 1024)

            # CreeazÄƒ backup info
            backup_info = {
                "folder": str(folder_path),
                "priority_file": str(folder_info["priority_file"]),
                "search_title": folder_info["search_title"],
                "identifier_base": folder_info["identifier_base"],
                "all_files": [str(f) for f in folder_info["all_files"]],
                "size_mb": round(size_mb, 2),
                "deleted_at": datetime.now().isoformat(),
                "reason": reason
            }

            # È˜terge folderul
            shutil.rmtree(folder_path)

            # ActualizeazÄƒ statisticile
            if "deleted_folders" not in self.state:
                self.state["deleted_folders"] = []

            self.state["deleted_folders"].append(backup_info)
            self.deleted_count += 1

            if "stats" not in self.state:
                self.state["stats"] = {
                    "total_checked": 0, "total_deleted": 0, "total_space_saved_mb": 0.0,
                    "search_hits": 0, "api_hits": 0, "url_test_hits": 0
                }

            self.state["stats"]["total_deleted"] += 1
            self.state["stats"]["total_space_saved_mb"] = round(
                self.state["stats"]["total_space_saved_mb"] + size_mb, 2
            )

            logger.info(f"âœ… Folder È™ters cu succes! ({size_mb:.2f} MB eliberat)")

            # SalveazÄƒ starea imediat
            self.save_state()

        except Exception as e:
            logger.error(f"âŒ Eroare la È™tergerea folderului: {e}")
            self.error_count += 1

    def generate_report(self):
        """GenereazÄƒ raportul final"""
        total_processed = self.checked_count + self.deleted_count

        report_lines = [
            "\n" + "=" * 60,
            "ğŸ“Š RAPORT FINAL - Pure Web Archive Checker",
            "=" * 60,
            f"âœ… Foldere verificate: {self.checked_count}",
            f"ğŸ—‘ï¸ Foldere È™terse (duplicate): {self.deleted_count}",
            f"âŒ Erori Ã®ntÃ¢mpinate: {self.error_count}",
            f"ğŸ’¾ SpaÈ›iu eliberat Ã®n aceastÄƒ sesiune: {sum(f.get('size_mb', 0) for f in self.state['deleted_folders'][-self.deleted_count:]):.2f} MB",
            "",
            "ğŸ“ˆ STATISTICI METODE DETECTARE:",
            f"ğŸ” Duplicate gÄƒsite prin cÄƒutare web: {self.search_hits}",
            f"ğŸ”§ Duplicate gÄƒsite prin API identifier: {self.api_hits}",
            f"ğŸŒ Duplicate gÄƒsite prin test direct URL: {self.url_test_hits}",
            "",
            "ğŸ“Š EFICIENÈšÄ‚ METODE:",
            f"ğŸ” CÄƒutare web: {(self.search_hits / max(total_processed, 1)) * 100:.1f}%",
            f"ğŸ”§ API identifier: {(self.api_hits / max(total_processed, 1)) * 100:.1f}%",
            f"ğŸŒ Test direct URL: {(self.url_test_hits / max(total_processed, 1)) * 100:.1f}%",
            "",
            f"ğŸ“ˆ STATISTICI TOTALE:",
            f"ğŸ“ˆ Total istoric foldere È™terse: {self.state['stats']['total_deleted']}",
            f"ğŸ“ˆ Total spaÈ›iu istoric eliberat: {self.state['stats']['total_space_saved_mb']:.2f} MB",
            "=" * 60
        ]

        for line in report_lines:
            logger.info(line)

        if self.deleted_count > 0:
            logger.info(f"\nğŸ“‹ FOLDERE È˜TERSE ÃN ACEASTÄ‚ SESIUNE:")
            recent_deleted = self.state["deleted_folders"][-self.deleted_count:]
            for i, folder_info in enumerate(recent_deleted, 1):
                logger.info(f"   {i}. {Path(folder_info['folder']).name}")
                logger.info(f"      Titlu: {folder_info.get('search_title', 'N/A')}")
                logger.info(f"      Motiv: {folder_info.get('reason', 'N/A')}")
                logger.info(f"      SpaÈ›iu eliberat: {folder_info.get('size_mb', 0):.2f} MB")

    def run(self):
        """RuleazÄƒ procesul principal"""
        logger.info("=" * 60)
        logger.info("ğŸš€ START - Archive.org Pure Web Duplicate Checker")
        logger.info(f"ğŸ“ Folder verificat: {ARCHIVE_PATH}")
        logger.info("âš¡ MetodÄƒ: 100% Web-based (fÄƒrÄƒ Chrome)")
        logger.info("ğŸ” 3 strategii: Search web + API identifier + Test direct URL")
        logger.info("=" * 60)

        # VerificÄƒ folderul sursÄƒ
        if not ARCHIVE_PATH.exists():
            logger.error(f"âŒ Folderul {ARCHIVE_PATH} nu existÄƒ!")
            return False

        try:
            # ScaneazÄƒ folderele
            folders_to_check = self.scan_folders()

            if not folders_to_check:
                logger.info("âœ… Nu sunt foldere de verificat!")
                return True

            # ProceseazÄƒ fiecare folder
            for i, folder_info in enumerate(folders_to_check, 1):
                logger.info(f"\nğŸ“Š Progres: {i}/{len(folders_to_check)}")

                try:
                    self.check_folder_web_only(folder_info)

                    # PauzÄƒ Ã®ntre verificÄƒri pentru a nu suprasolicita serverul
                    if i < len(folders_to_check):
                        logger.info(f"â³ PauzÄƒ {DELAY_BETWEEN_REQUESTS} secunde...")
                        time.sleep(DELAY_BETWEEN_REQUESTS)

                except KeyboardInterrupt:
                    logger.warning("\nâš ï¸ Ãntrerupt de utilizator")
                    break
                except Exception as e:
                    logger.error(f"âŒ Eroare la procesarea folderului: {e}")
                    continue

            # SalveazÄƒ starea finalÄƒ
            self.save_state()

            # GenereazÄƒ raportul
            self.generate_report()

            return True

        except KeyboardInterrupt:
            logger.warning("\nâš ï¸ Proces Ã®ntrerupt manual")
            self.save_state()
            self.generate_report()
            return False
        except Exception as e:
            logger.error(f"\nâŒ Eroare fatalÄƒ: {e}", exc_info=True)
            self.save_state()
            return False


def main():
    """FuncÈ›ia principalÄƒ"""
    try:
        checker = ArchivePureWebChecker()
        success = checker.run()

        if success:
            print("\nâœ… Proces finalizat cu succes!")
        else:
            print("\nâŒ Proces finalizat cu erori!")

    except Exception as e:
        print(f"âŒ Eroare fatalÄƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()