#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Script pentru verificarea È™i È™tergerea automatÄƒ a folderelor
care conÈ›in fiÈ™iere deja prezente pe Internet Archive.
FoloseÈ™te API-ul JSON al Archive.org pentru verificare rapidÄƒ.

È˜terge fiÈ™ierul archive_cleanup_state.json Ã®nainte de a rula scriptul, sau adaugÄƒ aceastÄƒ funcÈ›ie È™i apeleaz-o:

de ce nu imi spui niciodata unde anume sa adaug bucatile de cod? Daca este o functie noua care nu exista in codul precedent, si deci nu am cum sa o caut ni cod, te rog frumos sa imi spui unde anume sa o adaug in cod. DUpa ce linie sau intre ce linii, sau dupa ce functie sau intre ce functiii.
foarte bine. Pe viitor sa scrii functia noua, si sa-mi spui ca aici: AdaugÄƒ funcÈ›ia  def reset_all_state(self): - imediat DUPÄ‚ funcÈ›ia save_state È™i ÃNAINTE de funcÈ›ia clean_title_for_search . Asa e cel mai usor de inteles. Dar daca imi dau sa adaug linii noi intr-o functie, imi spui exact intre ce linii sa introduc acele linii noi, sau de unde pana unde trebuie sa le inlocuiesc.
"""

import os
import re
import time
import shutil
import json
import logging
import requests
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Any, Optional

# ============= CONFIGURÄ‚RI =============
ARCHIVE_PATH = Path(r"g:\ARHIVA\B")
DELAY_BETWEEN_SEARCHES = 2  # secunde Ã®ntre cÄƒutÄƒri pentru a nu suprasolicita serverul
STATE_FILE = Path("archive_cleanup_state.json")
LOG_FILE = Path(f"archive_cleanup_log_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt")
RELEVANT_EXTENSIONS = ['.pdf', '.epub', '.mobi', '.djvu', '.docx', '.doc', '.lit', '.rtf']
MAX_API_ATTEMPTS = 3
API_TIMEOUT = 15

# ============= CONFIGURARE LOGGER =============
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("ArchiveDuplicateChecker")


class ArchiveDuplicateChecker:
    def __init__(self):
        self.state = self.load_state()
        self.deleted_count = 0
        self.checked_count = 0
        self.error_count = 0
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'ArchiveDuplicateChecker/1.0 (Python Script)'
        })

    def load_state(self) -> Dict[str, Any]:
        """ÃncarcÄƒ starea salvatÄƒ anterior"""
        default_state = {
            "processed_folders": [],
            "deleted_folders": [],
            "last_processed": None,
            "stats": {
                "total_checked": 0,
                "total_deleted": 0,
                "total_space_saved_mb": 0.0
            }
        }

        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    state = json.load(f)

                    # IMPORTANT: AsigurÄƒ compatibilitate cu versiuni anterioare
                    # AdaugÄƒ cheile lipsÄƒ din starea veche
                    if "deleted_folders" not in state:
                        state["deleted_folders"] = []

                    if "stats" not in state:
                        state["stats"] = default_state["stats"]

                    if "processed_folders" not in state:
                        state["processed_folders"] = []

                    if "last_processed" not in state:
                        state["last_processed"] = None

                    logger.info(f"ğŸ“‹ Stare Ã®ncÄƒrcatÄƒ: {len(state.get('processed_folders', []))} foldere procesate anterior")
                    return state
            except Exception as e:
                logger.warning(f"âš ï¸ Nu s-a putut Ã®ncÄƒrca starea: {e}")
                logger.info("ğŸ“ Creez o stare nouÄƒ...")

        return default_state

    def save_state(self):
        """SalveazÄƒ starea curentÄƒ"""
        self.state["last_processed"] = datetime.now().isoformat()
        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump(self.state, f, indent=2, ensure_ascii=False)
            logger.debug("ğŸ’¾ Stare salvatÄƒ cu succes")
        except Exception as e:
            logger.error(f"âŒ Eroare la salvarea stÄƒrii: {e}")

    def reset_all_state(self):
        """ReseteazÄƒ complet toatÄƒ starea salvatÄƒ"""
        logger.warning("ğŸ”„ RESETARE COMPLETÄ‚ A STÄ‚RII!")
        self.state = {
            "processed_folders": [],
            "deleted_folders": [],
            "last_processed": None,
            "stats": {
                "total_checked": 0,
                "total_deleted": 0,
                "total_space_saved_mb": 0.0
            }
        }
        self.save_state()


    def clean_title_for_search(self, filename: str) -> str:
        """
        CurÄƒÈ›Äƒ numele fiÈ™ierului pentru cÄƒutare pe Internet Archive:
        - EliminÄƒ extensia
        - EliminÄƒ versiunile (v.0.9, v.0.9.8.5-161, v.0.9.8 MMXII, etc.)
        - EliminÄƒ sufixele (scan, ctrl, retail, cop1, etc.)
        - EliminÄƒ parantezele È™i conÈ›inutul lor
        - EliminÄƒ numerele de la Ã®nceput din titlu
        - PÄƒstreazÄƒ doar formatul "Nume, Prenume - Titlu"
        """
        # EliminÄƒ extensia
        name = Path(filename).stem

        logger.debug(f"ğŸ”§ CurÄƒÈ›are titlu original: {filename}")

        # EliminÄƒ sufixele de tipul _202508, _20250804, etc.
        name = re.sub(r'[_-]\d{6,8}$', '', name)

        # EliminÄƒ parantezele rotunde È™i tot conÈ›inutul lor
        name = re.sub(r'\s*\([^)]*\)', '', name)

        # EliminÄƒ È™i parantezele pÄƒtrate dacÄƒ existÄƒ
        name = re.sub(r'\s*\[[^\]]*\]', '', name)

        # EliminÄƒ versiunile complexe care pot apÄƒrea dupÄƒ " - "
        # AcoperÄƒ: v.0.9, v.0.9.8.5-161, v.0.9.8 MMXII, v.1.0, etc.
        name = re.sub(r'\s*[-â€“]\s*[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?(?:\s*[-â€“]\s*\d+)?$', '', name)

        # EliminÄƒ versiunile care pot apÄƒrea È™i fÄƒrÄƒ liniuÈ›Äƒ Ã®nainte
        name = re.sub(r'\s+[vV]\.?\s*[\d\.]+[\d\.\-]*(?:\s+[A-Z]+)?$', '', name)

        # Lista completÄƒ de sufixe de eliminat
        suffixes_to_remove = [
            'scan', 'ctrl', 'retail', r'cop\d+', 'Vp', 'draft', 'final',
            'ocr', 'OCR', 'edit', 'edited', 'rev', 'revised', 'proof',
            'beta', 'alpha', 'test', 'demo', 'sample', 'preview',
            'full', 'complete', 'fix', 'fixed', 'corrected'
        ]

        # ConstruieÈ™te pattern-ul pentru toate sufixele
        suffix_pattern = '|'.join(suffixes_to_remove)

        # EliminÄƒ sufixele care apar dupÄƒ ultima " - "
        parts = name.rsplit(' - ', 1)
        if len(parts) == 2:
            last_part = parts[1].strip()
            if re.match(f'^({suffix_pattern})$', last_part, re.IGNORECASE):
                name = parts[0]

        # EliminÄƒ numerele de la Ã®nceput din titlu (ex: "4. Comosicus" -> "Comosicus")
        if ' - ' in name:
            parts = name.split(' - ', 1)
            if len(parts) == 2:
                # CurÄƒÈ›Äƒ partea de titlu de numere de la Ã®nceput
                title = re.sub(r'^\d+\.\s*', '', parts[1])
                name = f"{parts[0]} - {title}"

        # CurÄƒÈ›Äƒ spaÈ›iile multiple È™i spaÈ›iile de la Ã®nceput/sfÃ¢rÈ™it
        name = re.sub(r'\s+', ' ', name).strip()

        # EliminÄƒ eventuale liniuÈ›e rÄƒmase la sfÃ¢rÈ™it
        name = re.sub(r'\s*[-â€“]\s*$', '', name)

        logger.debug(f"âœ¨ Titlu curÄƒÈ›at: {name}")

        return name

    def extract_base_name_for_identifier(self, title: str) -> List[str]:
        """Extrage pattern-uri simple È™i scurte pentru cÄƒutarea identifier-urilor pe Archive.org"""
        base_names = []

        # Ãncepe cu titlul original
        working_title = title.lower()

        # CurÄƒÈ›Äƒ È™i genereazÄƒ base name
        clean_name = re.sub(r'[^a-z0-9\s\.]', ' ', working_title)
        clean_name = re.sub(r'\s+', ' ', clean_name).strip()
        clean_name = clean_name.replace(' ', '-')

        # EliminÄƒm sufixele cunoscute pentru a avea un base name curat
        clean_name = re.sub(r'-(retail|scan|ctrl|cop\d+|v\d+.*?)$', '', clean_name)
        clean_name = re.sub(r'-#\w+$', '', clean_name)  # EliminÄƒ #ISTOR, etc.

        words = clean_name.split('-')

        # Genereaza variante de cÄƒutare Ã®ncepÃ¢nd cu cele mai simple
        # Pattern 1: Doar autorul (primele 2 cuvinte)
        if len(words) >= 2:
            base_names.append(f"{words[0]}-{words[1]}")

        # Pattern 2: Autor + primul cuvÃ¢nt din titlu (primele 3-4 cuvinte)
        for i in range(3, min(6, len(words) + 1)):
            pattern = '-'.join(words[:i])
            base_names.append(pattern)

        # Pattern 3: Variante mai lungi, dar nu peste 50 de caractere
        for length in [30, 40, 50, 60]:
            if len(clean_name) > length:
                # GÄƒseÈ™te ultima liniuÈ›Äƒ Ã®nainte de lungimea specificatÄƒ
                truncated = clean_name[:length]
                last_dash = truncated.rfind('-')
                if last_dash > length - 10:  # DacÄƒ liniuÈ›a e aproape de sfÃ¢rÈ™it
                    truncated = truncated[:last_dash]
                base_names.append(truncated)

        # EliminÄƒ duplicatele È™i pÄƒstreazÄƒ ordinea
        unique_names = []
        for name in base_names:
            if name and name not in unique_names and len(name) >= 10:  # Minim 10 caractere
                unique_names.append(name)

        logger.debug(f"ğŸ”§ Pattern-uri de cÄƒutare generate: {unique_names}")
        return unique_names

    def scan_folders(self) -> List[Dict[str, Any]]:
        """ScaneazÄƒ recursiv toate folderele È™i returneazÄƒ lista de fiÈ™iere de verificat"""
        tasks = []
        logger.info(f"ğŸ“‚ Scanez folderul: {ARCHIVE_PATH}")

        try:
            for root, dirs, files in os.walk(ARCHIVE_PATH):
                folder = Path(root)

                # GÄƒseÈ™te fiÈ™ierele relevante
                relevant_files = []
                for file in files:
                    if Path(file).suffix.lower() in RELEVANT_EXTENSIONS:
                        file_path = folder / file
                        relevant_files.append(file_path)

                if not relevant_files:
                    continue

                logger.debug(f"ğŸ“ Folder {folder.name}: {len(relevant_files)} fiÈ™iere relevante")

                # GrupeazÄƒ fiÈ™ierele dupÄƒ numele principal (fÄƒrÄƒ extensie È™i versiune)
                file_groups = {}
                for file_path in relevant_files:
                    clean_name = self.clean_title_for_search(file_path.name)
                    if clean_name not in file_groups:
                        file_groups[clean_name] = []
                    file_groups[clean_name].append(file_path)

                # AdaugÄƒ fiecare grup unic pentru verificare
                for clean_name, file_list in file_groups.items():
                    # CalculeazÄƒ dimensiunea totalÄƒ
                    total_size = sum(f.stat().st_size for f in file_list if f.exists())

                    tasks.append({
                        "search_title": clean_name,
                        "folder": folder,
                        "files": file_list,
                        "size": total_size
                    })

            logger.info(f"ğŸ“Š GÄƒsite {len(tasks)} titluri unice de verificat")
            return tasks

        except Exception as e:
            logger.error(f"âŒ Eroare la scanarea folderelor: {e}")
            return []

    def check_archive_api(self, title: str) -> bool:
        """VerificÄƒ dacÄƒ un titlu existÄƒ pe Internet Archive folosind API-ul JSON È™i cÄƒutare dupÄƒ identifier"""

        # Primul pas: cÄƒutare normalÄƒ dupÄƒ titlu
        if self._search_by_title(title):
            return True

        # Al doilea pas: cÄƒutare dupÄƒ identifier cu sufixe duplicate
        if self.check_for_duplicate_identifiers(title):
            return True

        return False

    def _search_by_title(self, title: str) -> bool:
        """CÄƒutare normalÄƒ dupÄƒ titlu (funcÈ›ia originalÄƒ)"""
        url = "https://archive.org/advancedsearch.php"

        # PregÄƒteÈ™te parametrii pentru API
        params = {
            "q": f'title:("{title}")',  # CÄƒutare exactÄƒ cu ghilimele
            "fl[]": ["identifier", "title"],  # ReturneazÄƒ ID È™i titlu
            "rows": 10,  # VerificÄƒ primele 10 rezultate
            "output": "json",
            "sort": "downloads desc"  # SorteazÄƒ dupÄƒ popularitate
        }

        logger.info(f"ğŸ” Caut pe Archive.org: '{title}'")
        logger.debug(f"   URL: {url}")
        logger.debug(f"   Parametri: {params}")

        for attempt in range(1, MAX_API_ATTEMPTS + 1):
            try:
                response = self.session.get(url, params=params, timeout=API_TIMEOUT)
                response.raise_for_status()

                data = response.json()
                response_data = data.get("response", {})
                num_found = response_data.get("numFound", 0)
                docs = response_data.get("docs", [])

                logger.debug(f"   ğŸ“Š Rezultate gÄƒsite: {num_found}")

                if num_found > 0:
                    # VerificÄƒ dacÄƒ vreun rezultat se potriveÈ™te
                    for doc in docs:
                        doc_title = doc.get("title", "")
                        doc_id = doc.get("identifier", "")

                        # CurÄƒÈ›Äƒ È™i titlul din rezultat pentru comparaÈ›ie
                        clean_doc_title = self.clean_title_for_search(doc_title)

                        # NormalizeazÄƒ pentru comparaÈ›ie
                        search_normalized = re.sub(r'[^a-z0-9\s]', '', title.lower())
                        search_normalized = re.sub(r'\s+', ' ', search_normalized).strip()

                        doc_normalized = re.sub(r'[^a-z0-9\s]', '', clean_doc_title.lower())
                        doc_normalized = re.sub(r'\s+', ' ', doc_normalized).strip()

                        # VerificÄƒ similaritatea
                        if self.are_titles_similar(search_normalized, doc_normalized):
                            logger.info(f"   âœ… GÄ‚SIT pe Archive.org prin cÄƒutare dupÄƒ titlu!")
                            logger.info(f"      ID: {doc_id}")
                            logger.info(f"      Titlu: {doc_title}")
                            return True

                    logger.info(f"   âŒ Nu s-a gÄƒsit potrivire exactÄƒ dupÄƒ titlu dintre {num_found} rezultate")
                else:
                    logger.info(f"   â„¹ï¸ Niciun rezultat gÄƒsit dupÄƒ titlu")

                return False

            except requests.RequestException as e:
                logger.warning(f"   âš ï¸ Eroare API la cÄƒutare titlu (Ã®ncercarea {attempt}/{MAX_API_ATTEMPTS}): {e}")
                if attempt < MAX_API_ATTEMPTS:
                    time.sleep(2 ** attempt)
            except json.JSONDecodeError as e:
                logger.error(f"   âŒ Eroare la parsarea rÄƒspunsului JSON: {e}")
                return False

        logger.error(f"   âŒ CÄƒutare titlu eÈ™uatÄƒ dupÄƒ {MAX_API_ATTEMPTS} Ã®ncercÄƒri")
        return False

    def are_titles_similar(self, title1: str, title2: str) -> bool:
        """VerificÄƒ dacÄƒ douÄƒ titluri sunt similare (tolerant la mici diferenÈ›e)"""
        # EliminÄƒ toate spaÈ›iile pentru comparaÈ›ie
        t1 = title1.replace(" ", "")
        t2 = title2.replace(" ", "")

        # VerificÄƒ egalitate exactÄƒ
        if t1 == t2:
            return True

        # VerificÄƒ dacÄƒ unul este conÈ›inut Ã®n celÄƒlalt
        if len(t1) > 3 and len(t2) > 3:  # Minim 4 caractere
            if t1 in t2 or t2 in t1:
                return True

        # VerificÄƒ dacÄƒ toate cuvintele din titlul cÄƒutat sunt Ã®n rezultat
        words1 = set(title1.split())
        words2 = set(title2.split())

        # EliminÄƒ cuvintele scurte (articole, prepoziÈ›ii)
        words1 = {w for w in words1 if len(w) > 2}
        words2 = {w for w in words2 if len(w) > 2}

        if words1 and words2:
            # VerificÄƒ dacÄƒ cel puÈ›in 80% din cuvinte se potrivesc
            common_words = words1.intersection(words2)
            if len(common_words) / len(words1) >= 0.8:
                return True

        return False

    def check_for_duplicate_identifiers(self, title: str) -> bool:
        """VerificÄƒ dacÄƒ existÄƒ identifier-uri cu sufixe duplicate pe Internet Archive"""

        # Primul pas: generez identifier-ul probabil È™i testez direct URL-ul
        base_names = self.extract_base_name_for_identifier(title)

        if base_names:
            # Iau cel mai lung base name È™i testez cu sufixe comune
            primary_base = base_names[-1]  # Cel mai lung/detaliat

            # Testez sufixe comune de duplicate
            common_suffixes = ['_202508', '_20250806', '_202507', '_20250805', '_202506']

            logger.info(f"ğŸ¯ TESTEZ DIRECT URL-uri pentru sufixe duplicate:")
            for suffix in common_suffixes:
                test_identifier = f"{primary_base}{suffix}"
                archive_url = f"https://archive.org/details/{test_identifier}"

                logger.info(f"   ğŸ”— Testez: {test_identifier}")

                try:
                    response = self.session.head(archive_url, timeout=8, allow_redirects=True)

                    if response.status_code == 200:
                        logger.info(f"   âœ… GÄ‚SIT DUPLICAT prin URL direct!")
                        logger.info(f"      URL existent: {archive_url}")
                        logger.info(f"      Status: {response.status_code}")
                        return True
                    elif response.status_code == 404:
                        logger.debug(f"   âŒ NU existÄƒ: {test_identifier}")
                    else:
                        logger.debug(f"   âš ï¸ Status {response.status_code} pentru: {test_identifier}")

                except requests.RequestException as e:
                    logger.debug(f"   âŒ Eroare pentru {test_identifier}: {e}")

                time.sleep(0.5)  # PauzÄƒ Ã®ntre teste

        # Al doilea pas: cÄƒutare prin API (ca Ã®nainte) - doar dacÄƒ testul direct a eÈ™uat
        logger.info(f"ğŸ” Testez È™i prin API-ul de cÄƒutare...")

        if not base_names:
            logger.debug("   âš ï¸ Nu s-au putut extrage base names pentru identifier")
            return False

        url = "https://archive.org/advancedsearch.php"

        # Testez doar primele 3 pattern-uri pentru a fi mai rapid
        for i, base_name in enumerate(base_names[:3], 1):
            logger.debug(f"   ğŸ” API pattern {i}/3: '{base_name}*'")

            params = {
                "q": f'identifier:({base_name}*)',
                "fl[]": ["identifier"],
                "rows": 50,
                "output": "json"
            }

            try:
                response = self.session.get(url, params=params, timeout=API_TIMEOUT)
                response.raise_for_status()

                data = response.json()
                response_data = data.get("response", {})
                num_found = response_data.get("numFound", 0)
                docs = response_data.get("docs", [])

                if num_found > 0:
                    logger.debug(f"      ğŸ“Š GÄƒsite {num_found} identifier-uri prin API")
                    for doc in docs:
                        identifier = doc.get("identifier", "")
                        if self._has_date_suffix(identifier):
                            logger.info(f"   âœ… GÄ‚SIT DUPLICAT prin API!")
                            logger.info(f"      Identifier cu sufix: {identifier}")
                            return True
                else:
                    logger.debug(f"      â„¹ï¸ Niciun rezultat prin API pentru '{base_name}*'")

            except Exception as e:
                logger.debug(f"      âŒ Eroare API pentru pattern {i}: {e}")

            time.sleep(0.5)

        logger.info(f"   âŒ Nu s-au gÄƒsit duplicate prin nicio metodÄƒ")
        return False

    def _has_date_suffix(self, identifier: str) -> bool:
        """VerificÄƒ dacÄƒ un identifier are sufix de datÄƒ (indicÃ¢nd un duplicat)"""
        # Pattern-uri de sufixe care indicÄƒ duplicate
        date_patterns = [
            r'_\d{6}$',          # _202508
            r'_\d{8}$',          # _20250806
            r'_\d{4}\d{2}$',     # _202508 (alt format)
            r'_\d{4}\d{2}\d{2}$' # _20250806 (alt format)
        ]

        for pattern in date_patterns:
            if re.search(pattern, identifier):
                logger.debug(f"      âœ… Sufix de datÄƒ gÄƒsit Ã®n '{identifier}': pattern {pattern}")
                return True

        return False

    def test_direct_search(self):
        """Test direct pentru identifier-ul cunoscut"""
        logger.info("ğŸ§ª TEST DIRECT - caut identifier-ul exact cunoscut:")

        # Testez direct dacÄƒ identifier-ul existÄƒ
        test_identifier = "berthon-simon-razboi-intre-aliati.-povestea-rivalitatii-dintre-churchill-rooseve_202508"

        url = "https://archive.org/advancedsearch.php"
        params = {
            "q": f'identifier:({test_identifier})',
            "fl[]": ["identifier"],
            "rows": 1,
            "output": "json"
        }

        try:
            response = self.session.get(url, params=params, timeout=API_TIMEOUT)
            response.raise_for_status()
            data = response.json()

            response_data = data.get("response", {})
            num_found = response_data.get("numFound", 0)
            docs = response_data.get("docs", [])

            logger.info(f"   ğŸ¯ CÄƒutare directÄƒ pentru '{test_identifier[:50]}...': {num_found} rezultate")

            if docs:
                for doc in docs:
                    logger.info(f"      âœ… GÄ‚SIT: {doc.get('identifier', '')}")
            else:
                logger.info(f"   âŒ Identifier-ul exact NU a fost gÄƒsit Ã®n Archive.org")
                logger.info(f"   ğŸ’¡ Probabil nu este Ã®ncÄƒ indexat sau nu este public")

        except Exception as e:
            logger.error(f"   âŒ Eroare la testul direct: {e}")

    def test_direct_url_access(self):
        """Testez direct dacÄƒ URL-ul Archive.org rÄƒspunde (chiar dacÄƒ nu e Ã®n cÄƒutare)"""
        logger.info("ğŸŒ TEST DIRECT URL - verific dacÄƒ pagina Archive.org existe:")

        test_identifier = "berthon-simon-razboi-intre-aliati.-povestea-rivalitatii-dintre-churchill-rooseve_202508"
        archive_url = f"https://archive.org/details/{test_identifier}"

        logger.info(f"   ğŸ”— Testez URL-ul: {archive_url}")

        try:
            # Folosesc HEAD request pentru a nu descÄƒrca toatÄƒ pagina
            response = self.session.head(archive_url, timeout=10, allow_redirects=True)

            logger.info(f"   ğŸ“¡ Status HTTP: {response.status_code}")
            logger.info(f"   ğŸ“‹ Headers: {dict(list(response.headers.items())[:5])}")  # Primele 5 header-uri

            if response.status_code == 200:
                logger.info(f"   âœ… URL-UL EXISTÄ‚ pe Archive.org!")
                logger.info(f"   ğŸ¯ CONFIRMAT: FiÈ™ierul existÄƒ Ã®n sistem (chiar dacÄƒ nu e Ã®n cÄƒutare publicÄƒ)")
                return True
            elif response.status_code == 404:
                logger.info(f"   âŒ URL-ul NU existÄƒ (404 Not Found)")
                return False
            else:
                logger.info(f"   âš ï¸ Status neaÈ™teptat: {response.status_code}")
                return False

        except requests.RequestException as e:
            logger.error(f"   âŒ Eroare la testarea URL-ului: {e}")
            return False

    def _matches_duplicate_pattern(self, identifier: str, base_name: str) -> bool:
        """VerificÄƒ dacÄƒ un identifier se potriveÈ™te cu pattern-ul de duplicat"""
        # VerificÄƒ dacÄƒ identifier-ul Ã®ncepe cu base_name
        if not identifier.lower().startswith(base_name.lower()):
            return False

        # Extrage sufixul
        suffix = identifier[len(base_name):]

        # VerificÄƒ pattern-urile de duplicat
        # Pattern 1: _YYYYMM (ex: _202508)
        if re.match(r'^_\d{6}$', suffix):
            logger.debug(f"      âœ… GÄƒsit pattern YYYYMM: {suffix}")
            return True

        # Pattern 2: _YYYYMMDD (ex: _20250806)
        if re.match(r'^_\d{8}$', suffix):
            logger.debug(f"      âœ… GÄƒsit pattern YYYYMMDD: {suffix}")
            return True

        # Pattern 3: verificÄƒ È™i alte variante comune de duplicat
        # De exemplu: -v2, -copy, -duplicate, etc.
        duplicate_patterns = [
            r'^[-_]v\d+$',           # -v1, _v2, etc.
            r'^[-_]copy\d*$',        # -copy, _copy1, etc.
            r'^[-_]duplicate\d*$',   # -duplicate, _duplicate1, etc.
            r'^[-_]\d+$'             # -1, _2, etc.
        ]

        for pattern in duplicate_patterns:
            if re.match(pattern, suffix, re.IGNORECASE):
                logger.debug(f"      âœ… GÄƒsit pattern duplicat: {suffix}")
                return True

        return False

    def delete_folder(self, task: Dict[str, Any]):
        """È˜terge un folder È™i toate subfiÈ™ierele sale"""
        folder = task['folder']

        # DeterminÄƒ ce folder sÄƒ È™tergem
        if folder.parent != ARCHIVE_PATH:
            folder_to_delete = folder.parent
        else:
            folder_to_delete = folder

        try:
            logger.warning(f"ğŸ—‘ï¸ È˜terg folderul: {folder_to_delete}")
            logger.info(f"   Motivul: FiÈ™ierul '{task['search_title']}' existÄƒ pe Archive.org")

            # CalculeazÄƒ spaÈ›iul eliberat (Ã®n MB)
            size_mb = task.get('size', 0) / (1024 * 1024)

            # CreeazÄƒ backup Ã®n state Ã®nainte de È™tergere
            backup_info = {
                "folder": str(folder_to_delete),
                "title": task['search_title'],
                "files": [str(f) for f in task['files']],
                "size_mb": round(size_mb, 2),
                "deleted_at": datetime.now().isoformat()
            }

            # È˜terge folderul
            shutil.rmtree(folder_to_delete)

            # AsigurÄƒ cÄƒ deleted_folders existÄƒ Ã®nainte de a adÄƒuga
            if "deleted_folders" not in self.state:
                self.state["deleted_folders"] = []

            # ActualizeazÄƒ statisticile
            self.state["deleted_folders"].append(backup_info)
            self.deleted_count += 1

            # AsigurÄƒ cÄƒ stats existÄƒ
            if "stats" not in self.state:
                self.state["stats"] = {
                    "total_checked": 0,
                    "total_deleted": 0,
                    "total_space_saved_mb": 0.0
                }

            self.state["stats"]["total_deleted"] += 1
            self.state["stats"]["total_space_saved_mb"] = round(
                self.state["stats"]["total_space_saved_mb"] + size_mb, 2
            )

            logger.info(f"   âœ… Folder È™ters cu succes! ({size_mb:.2f} MB eliberat)")

            # SalveazÄƒ starea imediat dupÄƒ È™tergere
            self.save_state()

            return True

        except Exception as e:
            logger.error(f"   âŒ Eroare la È™tergerea folderului: {e}")
            self.error_count += 1
            return False

    def reset_processed_folders(self):
        """ReseteazÄƒ lista de foldere procesate pentru a le verifica din nou"""
        logger.info("ğŸ”„ Resetez lista de foldere procesate pentru reverificare...")
        self.state["processed_folders"] = []
        self.save_state()

    def generate_report(self):
        """GenereazÄƒ un raport final detaliat"""
        report_lines = [
            "\n" + "=" * 60,
            "ğŸ“Š RAPORT FINAL",
            "=" * 60,
            f"âœ… FiÈ™iere verificate: {self.checked_count}",
            f"ğŸ—‘ï¸ Foldere È™terse: {self.deleted_count}",
            f"âŒ Erori Ã®ntÃ¢mpinate: {self.error_count}",
            f"ğŸ’¾ SpaÈ›iu total eliberat: {self.state['stats']['total_space_saved_mb']:.2f} MB",
            f"ğŸ“ˆ Total istoric foldere È™terse: {self.state['stats']['total_deleted']}",
            "=" * 60
        ]

        for line in report_lines:
            logger.info(line)

        if self.state["deleted_folders"]:
            logger.info("\nğŸ“‹ FOLDERE È˜TERSE ÃN ACEASTÄ‚ SESIUNE:")
            for i, folder_info in enumerate(self.state["deleted_folders"][-self.deleted_count:], 1):
                logger.info(f"   {i}. {folder_info['folder']}")
                logger.info(f"      Titlu: {folder_info['title']}")
                logger.info(f"      SpaÈ›iu eliberat: {folder_info.get('size_mb', 0):.2f} MB")

    def run(self):
        """RuleazÄƒ procesul principal"""
        logger.info("=" * 60)
        logger.info("ğŸš€ START - Internet Archive Duplicate Checker")
        logger.info(f"ğŸ“ Folder verificat: {ARCHIVE_PATH}")
        logger.info(f"â±ï¸ Delay Ã®ntre cÄƒutÄƒri: {DELAY_BETWEEN_SEARCHES} secunde")
        logger.info("=" * 60)

        # VerificÄƒ dacÄƒ folderul existÄƒ
        if not ARCHIVE_PATH.exists():
            logger.error(f"âŒ Folderul {ARCHIVE_PATH} nu existÄƒ!")
            return False

        # ReseteazÄƒ folderele procesate pentru a le verifica din nou
        self.reset_processed_folders()
        # ÃnlocuieÈ™te linia self.test_direct_search() cu aceste douÄƒ linii:
        self.test_direct_search()
        self.test_direct_url_access()

        try:
            # ScaneazÄƒ folderele
            tasks = self.scan_folders()

            if not tasks:
                logger.info("âœ… Nu sunt fiÈ™iere de verificat!")
                return True

            # ProceseazÄƒ fiecare task
            for i, task in enumerate(tasks, 1):
                folder_str = str(task["folder"])

                # Sari peste folderele deja procesate Ã®n aceastÄƒ sesiune
                if folder_str in self.state["processed_folders"]:
                    logger.debug(f"â­ï¸ Folder deja procesat: {folder_str}")
                    continue

                logger.info(f"\nğŸ“Š Progres: {i}/{len(tasks)}")
                logger.info(f"ğŸ“‚ Folder: {task['folder'].name}")
                logger.info(f"ğŸ“„ Titlu cÄƒutat: {task['search_title']}")

                # VerificÄƒ pe Archive.org
                found = self.check_archive_api(task['search_title'])

                if found:
                    # È˜terge folderul
                    self.delete_folder(task)
                else:
                    logger.info(f"   â„¹ï¸ PÄƒstrez folderul - nu existÄƒ pe Archive.org")

                # MarcheazÄƒ ca procesat
                self.state["processed_folders"].append(folder_str)
                self.checked_count += 1
                self.state["stats"]["total_checked"] += 1

                # SalveazÄƒ starea periodic (la fiecare 5 foldere)
                if i % 5 == 0:
                    self.save_state()

                # AÈ™teaptÄƒ Ã®ntre cÄƒutÄƒri (exceptÃ¢nd ultima)
                if i < len(tasks):
                    logger.info(f"â³ AÈ™tept {DELAY_BETWEEN_SEARCHES} secunde...")
                    time.sleep(DELAY_BETWEEN_SEARCHES)

            # SalveazÄƒ starea finalÄƒ
            self.save_state()

            # GenereazÄƒ raportul final
            self.generate_report()

            return True

        except KeyboardInterrupt:
            logger.warning("\nâš ï¸ Proces Ã®ntrerupt de utilizator")
            self.save_state()
            self.generate_report()
            return False

        except Exception as e:
            logger.error(f"\nâŒ Eroare fatalÄƒ: {e}", exc_info=True)
            self.save_state()
            return False


def main():
    """FuncÈ›ia principalÄƒ"""
    try:
        checker = ArchiveDuplicateChecker()
        success = checker.run()

        if success:
            print("\nâœ… Proces finalizat cu succes!")
        else:
            print("\nâŒ Proces finalizat cu erori!")

    except Exception as e:
        print(f"âŒ Eroare fatalÄƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()